{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/justheuristic/4c82ef4d448ce62cb5459484f66f56aa/practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H91Iz3PiAFEK"
   },
   "source": [
    "### Practice 1: Parallel GloVe\n",
    "\n",
    "In this assignment we'll build parallel GloVe training from scratch. Well, almost from scratch:\n",
    "* we'll use python's builtin [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) library\n",
    "* and learn to access numpy arrays from multiple processes!\n",
    "\n",
    "![img](https://i.imgur.com/YHluIBo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F9iWJGzIAFEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_NUM_THREADS=1\n",
      "env: NUMEXPR_NUM_THREADS=1\n",
      "env: OMP_NUM_THREADS=1\n",
      "Requirement already satisfied: nltk in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: datasets in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (1.18.3)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (4.63.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (0.2.1)\n",
      "Requirement already satisfied: packaging in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: aiohttp in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: pyyaml in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/d2l/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "--2022-03-07 00:51:48--  https://raw.githubusercontent.com/mryab/efficient-dl-systems/main/week02_distributed/utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1071 (1.0K) [text/plain]\n",
      "Saving to: 'utils.py'\n",
      "\n",
      "utils.py            100%[===================>]   1.05K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-03-07 00:51:48 (46.6 MB/s) - 'utils.py' saved [1071/1071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1\n",
    "# set numpy to single-threaded mode for benchmarking\n",
    "\n",
    "!pip install --upgrade nltk datasets tqdm\n",
    "!wget https://raw.githubusercontent.com/mryab/efficient-dl-systems/main/week02_distributed/utils.py -O utils.py\n",
    "\n",
    "import time, random\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                    x86_64\n",
      "CPU op-mode(s):                  32-bit, 64-bit\n",
      "Byte Order:                      Little Endian\n",
      "Address sizes:                   46 bits physical, 48 bits virtual\n",
      "CPU(s):                          4\n",
      "On-line CPU(s) list:             0-3\n",
      "Thread(s) per core:              2\n",
      "Core(s) per socket:              2\n",
      "Socket(s):                       1\n",
      "NUMA node(s):                    1\n",
      "Vendor ID:                       GenuineIntel\n",
      "CPU family:                      6\n",
      "Model:                           85\n",
      "Model name:                      Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "Stepping:                        7\n",
      "CPU MHz:                         2499.998\n",
      "BogoMIPS:                        4999.99\n",
      "Hypervisor vendor:               KVM\n",
      "Virtualization type:             full\n",
      "L1d cache:                       64 KiB\n",
      "L1i cache:                       64 KiB\n",
      "L2 cache:                        2 MiB\n",
      "L3 cache:                        35.8 MiB\n",
      "NUMA node0 CPU(s):               0-3\n",
      "Vulnerability Itlb multihit:     KVM: Vulnerable\n",
      "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
      "Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no mic\n",
      "                                 rocode; SMT Host state unknown\n",
      "Vulnerability Meltdown:          Mitigation; PTI\n",
      "Vulnerability Spec store bypass: Vulnerable\n",
      "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user\n",
      "                                  pointer sanitization\n",
      "Vulnerability Spectre v2:        Mitigation; Full generic retpoline, STIBP disab\n",
      "                                 led, RSB filling\n",
      "Vulnerability Srbds:             Not affected\n",
      "Vulnerability Tsx async abort:   Not affected\n",
      "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n",
      "                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n",
      "                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n",
      "                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n",
      "                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n",
      "                                 d sse4_1 sse4_2 x2apic movbe popcnt tsc_deadlin\n",
      "                                 e_timer aes xsave avx f16c rdrand hypervisor la\n",
      "                                 hf_lm abm 3dnowprefetch invpcid_single pti fsgs\n",
      "                                 base tsc_adjust bmi1 avx2 smep bmi2 erms invpci\n",
      "                                 d mpx avx512f avx512dq rdseed adx smap clflusho\n",
      "                                 pt clwb avx512cd avx512bw avx512vl xsaveopt xsa\n",
      "                                 vec xgetbv1 xsaves ida arat pku ospke\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVojuqhGAFES"
   },
   "source": [
    "### Multiprocessing basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cr6HV2PjAFEU"
   },
   "outputs": [],
   "source": [
    "def foo(i):\n",
    "    \"\"\" Imagine particularly computation-heavy function... \"\"\"\n",
    "    print(end=f\"Began foo({i})...\\n\")\n",
    "    result = np.sin(i)\n",
    "    time.sleep(abs(result))\n",
    "    print(end=f\"Finished foo({i}) = {result:.3f}.\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hOOPAmDdAFEW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Began foo(0)...\n",
      "Finished foo(0) = 0.000.\n",
      "Began foo(1)...\n",
      "Finished foo(1) = 0.841.\n",
      "Began foo(2)...\n",
      "Finished foo(2) = 0.909.\n",
      "Began foo(3)...\n",
      "Finished foo(3) = 0.141.\n",
      "Began foo(4)...\n",
      "Finished foo(4) = -0.757.\n",
      "Began foo(5)...\n",
      "Finished foo(5) = -0.959.\n",
      "Began foo(6)...\n",
      "Finished foo(6) = -0.279.\n",
      "Began foo(7)...\n",
      "Finished foo(7) = 0.657.\n",
      "Began foo(8)...\n",
      "Finished foo(8) = 0.989.\n",
      "Began foo(9)...\n",
      "Finished foo(9) = 0.412.\n",
      "CPU times: user 12.2 ms, sys: 309 µs, total: 12.5 ms\n",
      "Wall time: 5.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_naive = [foo(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzBc-wA2AFEX"
   },
   "source": [
    "Same, but with multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TQPXLqdlAFEY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 processes!\n",
      "Began foo(0)...\n",
      "Finished foo(0) = 0.000.\n",
      "Began foo(1)...\n",
      "Began foo(3)...\n",
      "Began foo(2)...\n",
      "Began foo(4)...\n",
      "Began foo(5)...\n",
      "Began foo(6)...\n",
      "Began foo(7)...\n",
      "Began foo(8)...\n",
      "Began foo(9)...\n",
      "Finished foo(3) = 0.141.\n",
      "Finished foo(6) = -0.279.\n",
      "Finished foo(9) = 0.412.\n",
      "Finished foo(7) = 0.657.\n",
      "Finished foo(4) = -0.757.\n",
      "Finished foo(1) = 0.841.\n",
      "Finished foo(2) = 0.909.\n",
      "Finished foo(5) = -0.959.\n",
      "Finished foo(8) = 0.989.\n",
      "CPU times: user 13.3 ms, sys: 64.6 ms, total: 77.9 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processes = []\n",
    "for i in range(10):\n",
    "    proc = mp.Process(target=foo, args=[i])\n",
    "    processes.append(proc)\n",
    "\n",
    "print(f\"Created {len(processes)} processes!\")\n",
    "\n",
    "# start in parallel\n",
    "for proc in processes:\n",
    "    proc.start()\n",
    "    \n",
    "# wait for everyone finish\n",
    "for proc in processes:\n",
    "    proc.join()  # wait until proc terminates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-e91ez4AFEZ"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Great! But how do we collect the values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJCsKVJsAFEb"
   },
   "source": [
    "__Solution 1:__ with pipes!\n",
    "\n",
    "Two \"sides\", __one__ process from each side\n",
    "* `pipe_side.send(data)` - throw data into the pipe (do not wait for it to be read)\n",
    "* `data = pipe_side.recv()` - read data. If there is none, wait for someone to send data\n",
    "\n",
    "__Rules:__\n",
    "* each side should be controlled by __one__ process\n",
    "* data transferred through pipes must be serializable\n",
    "* if `duplex=True`, processes can communicate both ways\n",
    "* if `duplex=False`, \"left\" receives and \"right\" side sends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lRI8OvzwAFEd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "side_B.recv() ->  123\n",
      "side_B.recv() ->  {'ololo': array([0.37108177, 0.46903699, 1.59422099])}\n"
     ]
    }
   ],
   "source": [
    "side_A, side_B = mp.Pipe()\n",
    "\n",
    "side_A.send(123)\n",
    "side_A.send({'ololo': np.random.randn(3)})\n",
    "\n",
    "print(\"side_B.recv() -> \", side_B.recv())\n",
    "print(\"side_B.recv() -> \", side_B.recv())\n",
    "\n",
    "# note: calling recv a third will hang the process (waiting for someone to send data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KCKp1tq1AFEe"
   },
   "outputs": [],
   "source": [
    "def compute_and_send(i, output_pipe):\n",
    "    print(end=f\"Began compute_and_send({i})...\\n\")\n",
    "    result = np.sin(i)\n",
    "    time.sleep(abs(result))\n",
    "    print(end=f\"Finished compute_and_send({i}) = {result:.3f}.\\n\")\n",
    "    \n",
    "    output_pipe.send(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3j1mKkn5AFEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Began compute_and_send(0)...\n",
      "Finished compute_and_send(0) = 0.000.\n",
      "Began compute_and_send(1)...\n",
      "Began compute_and_send(2)...\n",
      "Began compute_and_send(4)...\n",
      "Began compute_and_send(3)...\n",
      "Began compute_and_send(5)...\n",
      "Began compute_and_send(6)...\n",
      "Began compute_and_send(7)...\n",
      "Began compute_and_send(8)...\n",
      "Began compute_and_send(9)...\n",
      "MAIN PROCESS: awaiting results...\n",
      "MAIN_PROCESS: received 0.0\n",
      "Finished compute_and_send(3) = 0.141.\n",
      "Finished compute_and_send(6) = -0.279.\n",
      "Finished compute_and_send(9) = 0.412.\n",
      "Finished compute_and_send(7) = 0.657.\n",
      "Finished compute_and_send(4) = -0.757.\n",
      "Finished compute_and_send(1) = 0.841.\n",
      "Finished compute_and_send(2) = 0.909.\n",
      "Finished compute_and_send(5) = -0.959.\n",
      "Finished compute_and_send(8) = 0.989.\n",
      "MAIN_PROCESS: received 0.8414709848078965\n",
      "MAIN_PROCESS: received 0.9092974268256817\n",
      "MAIN_PROCESS: received 0.1411200080598672\n",
      "MAIN_PROCESS: received -0.7568024953079282\n",
      "MAIN_PROCESS: received -0.9589242746631385\n",
      "MAIN_PROCESS: received -0.27941549819892586\n",
      "MAIN_PROCESS: received 0.6569865987187891\n",
      "MAIN_PROCESS: received 0.9893582466233818\n",
      "MAIN_PROCESS: received 0.4121184852417566\n",
      "MAIN PROCESS: done!\n",
      "CPU times: user 23.2 ms, sys: 60.7 ms, total: 84 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_pipes = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    side_A, side_B = mp.Pipe(duplex=False)\n",
    "    # note: duplex=False means that side_B can only send\n",
    "    # and side_A can only recv. Otherwise its bidirectional\n",
    "    result_pipes.append(side_A)\n",
    "    proc = mp.Process(target=compute_and_send, args=[i, side_B])\n",
    "    proc.start()\n",
    "\n",
    "print(\"MAIN PROCESS: awaiting results...\")\n",
    "for pipe in result_pipes:\n",
    "    print(f\"MAIN_PROCESS: received {pipe.recv()}\")\n",
    "print(\"MAIN PROCESS: done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woOzj-h9AFEf"
   },
   "source": [
    "__Solution 2:__ with multiprocessing templates\n",
    "\n",
    "Multiprocessing contains some template data structures that help you communicate between processes.\n",
    "\n",
    "One such structure is `mp.Queue` a Queue that can be accessed by multiple processes in parallel.\n",
    "* `queue.put` adds the value to the queue, accessible by all other processes\n",
    "* `queue.get` returns the earliest added value and removes it from queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P8mPT8uvAFEg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: awaiting queue...\n",
      "proc_B1: putting more stuff into queue!\n",
      "A: retreived from queue: 0.417022004702574\n",
      "A: awaiting queue...\n",
      "proc_B2: putting more stuff into queue!\n",
      "A: retreived from queue: 0.43599490214200376\n",
      "A: done!\n"
     ]
    }
   ],
   "source": [
    "queue = mp.Queue()\n",
    "\n",
    "def func_A(queue):\n",
    "    print(\"A: awaiting queue...\")\n",
    "    print(\"A: retreived from queue:\", queue.get())\n",
    "    print(\"A: awaiting queue...\")\n",
    "    print(\"A: retreived from queue:\", queue.get())\n",
    "    print(\"A: done!\")\n",
    "\n",
    "def func_B(i, queue):\n",
    "    np.random.seed(i)\n",
    "    value = np.random.rand()\n",
    "    time.sleep(value)\n",
    "    print(f\"proc_B{i}: putting more stuff into queue!\")\n",
    "    queue.put(value)\n",
    "    \n",
    "\n",
    "proc_A = mp.Process(target=func_A, args=[queue])\n",
    "proc_A.start();\n",
    "\n",
    "proc_B1 = mp.Process(target=func_B, args=[1, queue])\n",
    "proc_B2 = mp.Process(target=func_B, args=[2, queue])\n",
    "proc_B1.start(), proc_B2.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-rzgIRPAFEh"
   },
   "source": [
    "__Important note:__ you can see that the two values above are identical.\n",
    "\n",
    "This is because proc_B1 and proc_B2 were forked (cloned) with __the same random state!__\n",
    "\n",
    "To mitigate this issue, run `np.random.seed()` in each process (same for torch, tensorflow).\n",
    "\n",
    "<details>\n",
    "    <summary>In fact, please go and to that <b>right now!</b></summary>\n",
    "    <img src='https://media.tenor.com/images/32c950f36a61ec7e5060f5eee9140396/tenor.gif' height=200px>\n",
    "</details>\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "__Less important note:__ `mp.Queue vs mp.Pipe`\n",
    "- pipes are much faster for 1v1 communication\n",
    "- queues support arbitrary number of processes\n",
    "- queues are implemented with pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHck3-qUAFEi"
   },
   "source": [
    "### GloVe preprocessing\n",
    "\n",
    "Before we can train GloVe, we must first construct the co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qNC7L4avAFEj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/studio-lab-user/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5cfb468506460a803e899ae08e31b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "data = datasets.load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "# for fast debugging, you can temporarily use smaller data: 'wikitext-2-raw-v1'\n",
    "\n",
    "print(\"Example:\", data['train']['text'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctIVLlsaAFEk"
   },
   "source": [
    "__First,__ let's build a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-3LTURX-AFEl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 459,\n",
       "         ',': 349,\n",
       "         '.': 225,\n",
       "         'of': 193,\n",
       "         'to': 150,\n",
       "         'and': 147,\n",
       "         'in': 104,\n",
       "         '@': 100,\n",
       "         'a': 93,\n",
       "         'was': 83})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "tokenizer = NLTKWordTokenizer()\n",
    "\n",
    "def count_tokens(lines, top_k=None):\n",
    "    \"\"\" Tokenize lines and return top_k most frequent tokens and their counts \"\"\"\n",
    "    sent_tokens = tokenizer.tokenize_sents(map(str.lower, lines))\n",
    "    token_counts = Counter([token for sent in sent_tokens for token in sent])\n",
    "    return Counter(dict(token_counts.most_common(top_k)))\n",
    "\n",
    "count_tokens(data['train']['text'][:100], top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YCyX3nruAFEm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 21.863430815006723 sec..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sequential algorithm\n",
    "from timeit import default_timer\n",
    "\n",
    "texts = data['train']['text'][:100_000]\n",
    "vocabulary_size = 32_000\n",
    "batch_size = 10_000\n",
    "\n",
    "token_counts = Counter()\n",
    "start = default_timer()\n",
    "for batch_start in trange(0, len(texts), batch_size):\n",
    "    batch_texts = texts[batch_start: batch_start + batch_size]\n",
    "    batch_counts = count_tokens(batch_texts, top_k=vocabulary_size)\n",
    "    token_counts += Counter(batch_counts)\n",
    "\n",
    "# save for later\n",
    "token_counts_reference = Counter(token_counts)\n",
    "\n",
    "end = default_timer()\n",
    "print(f'Took {end - start} sec..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nsYnwdFAFEo"
   },
   "source": [
    "### Let's parallelize (20% points)\n",
    "\n",
    "__Your task__ is to speed up the code above using using multiprocessing with queues and/or pipes _(or [shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html) if you're up to that)_.\n",
    "\n",
    "__Kudos__ for implementing some form of global progress tracker (like progressbar above)\n",
    "\n",
    "Please do **not** use task executors (e.g. mp.pool, joblib, ProcessPoolExecutor), we'll get to them soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k4IporOCAFEo"
   },
   "outputs": [],
   "source": [
    "texts = data['train']['text'][:100_000]\n",
    "vocabulary_size = 32_000\n",
    "batch_size = 10_000\n",
    "\n",
    "def count_step(texts, slice_index, batch_size, output_pipe):\n",
    "    token_counts = Counter()\n",
    "\n",
    "    for batch_start in trange(slice_index[0], slice_index[1], batch_size):\n",
    "        batch_end = min(batch_start + batch_size, slice_index[1])\n",
    "        batch_texts = texts[batch_start: batch_end]\n",
    "        batch_counts = count_tokens(batch_texts, top_k=vocabulary_size)\n",
    "        token_counts += Counter(batch_counts)\n",
    "\n",
    "    output_pipe.send(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def straight_slice_indexes(slices_indexes, batch_size):\n",
    "    last_ix = slices_indexes[-1][-1]\n",
    "    for i in range(len(slices_indexes)):\n",
    "        for j in range(len(slices_indexes[i])):            \n",
    "            if slices_indexes[i][j] % batch_size != 0:\n",
    "                slices_indexes[i][j] = ((slices_indexes[i][j] // batch_size) + 1) * batch_size\n",
    "                \n",
    "    slices_indexes[-1][-1] = min(slices_indexes[-1][-1], last_ix)\n",
    "    return slices_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 30], [30, 50], [50, 80], [80, 100]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "straight_slice_indexes([[0, 25], [25, 50], [50, 75], [75, 100]], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_counts_parallel(texts, vocabulary_size, batch_size):\n",
    "    n_cpu = n_slices = mp.cpu_count()\n",
    "    slice_size = len(texts) // n_slices\n",
    "    slices_indexes = [[i * slice_size,(i + 1) * slice_size] for i in range(n_cpu)]\n",
    "    print('Prepared indexes:', slices_indexes)\n",
    "    slices_indexes = straight_slice_indexes(slices_indexes, batch_size)\n",
    "    print('Aligned indexes:', slices_indexes)\n",
    "\n",
    "    result_pipes = []\n",
    "    start = default_timer()\n",
    "    \n",
    "    for i in range(n_cpu):\n",
    "        side_A, side_B = mp.Pipe(duplex=False)\n",
    "        result_pipes.append(side_A)\n",
    "        proc_i = mp.Process(target=count_step, args=[texts, slices_indexes[i], batch_size, side_B])\n",
    "        proc_i.start()\n",
    "        # processes.append(proc_i)\n",
    "\n",
    "    result = Counter()\n",
    "    print(\"Awaiting pipe_results...\")\n",
    "\n",
    "    for pipe in result_pipes:\n",
    "        r = pipe.recv()\n",
    "        result += r\n",
    "    print(\"Finished waiting!\")\n",
    "\n",
    "    end = default_timer()\n",
    "    print(f'Took {end - start} sec..')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared indexes: [[0, 25000], [25000, 50000], [50000, 75000], [75000, 100000]]\n",
      "Aligned indexes: [[0, 30000], [30000, 50000], [50000, 80000], [80000, 100000]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awaiting pipe_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.81s/it]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.29s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished waiting!\n",
      "Took 10.25968261800881 sec..\n"
     ]
    }
   ],
   "source": [
    "texts = data['train']['text'][:100_000]\n",
    "vocabulary_size = 32_000\n",
    "batch_size = 10_000\n",
    "\n",
    "\n",
    "token_counts = compute_token_counts_parallel(texts, vocabulary_size, batch_size)\n",
    "token_counts = Counter(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ygCy9JSKAFEo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "assert len(token_counts) == len(token_counts_reference)\n",
    "for token, ref_count in token_counts_reference.items():\n",
    "    assert token in token_counts, token\n",
    "    assert token_counts[token] == ref_count, token\n",
    "\n",
    "token_counts = Counter(dict(token_counts.most_common(vocabulary_size)))\n",
    "\n",
    "vocabulary = sorted(token_counts.keys())\n",
    "token_to_index = {token: i for i, token in enumerate(vocabulary)}\n",
    "assert len(vocabulary) == vocabulary_size, len(vocabulary)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgALuOgdAFEp"
   },
   "source": [
    "### Part 2: Construct co-occurence matrix (10% points)\n",
    "\n",
    "\n",
    "__Your task__ is to count co-occurences of all words in a 5-token window. Please use the same preprocessing and tokenizer as above.\n",
    "\n",
    "__Also:__ please only count words that are in the vocabulary defined above.\n",
    "\n",
    "![image.png](https://i.imgur.com/2XmhYn5.png)\n",
    "\n",
    "\n",
    "\n",
    "__Note:__ this task and everything below has no instructions/interfaces. We will design those interfaces __together on the seminar.__\n",
    "\n",
    "The detailed instructions will appear later this night after the seminar is over.\n",
    "However, if you want to write the code from scratch, feel free to ignore these instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1N1acrkUAFEq"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def count_token_cooccurences(lines, vocabulary_size: int, window_size: int):\n",
    "    \"\"\" Tokenize lines and return top_k most frequent tokens and their counts \"\"\"\n",
    "    cooc = Counter()\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = tokenizer.tokenize(line.lower())\n",
    "        token_ix = [token_to_index[token] for token in tokens\n",
    "                    if token in token_to_index]\n",
    "        \n",
    "        for i in range(len(token_ix)):\n",
    "            for j in range(max(i - window_size, 0),\n",
    "                           min(i + window_size + 1, len(token_ix))):\n",
    "                if i != j:\n",
    "                    cooc[token_ix[i], token_ix[j]] += 1 / abs(i - j)\n",
    "    return counter_to_matrix(cooc, vocabulary_size)\n",
    "\n",
    "def counter_to_matrix(counter, vocabulary_size):\n",
    "    keys, values = zip(*counter.items())\n",
    "    ii, jj = zip(*keys)\n",
    "    return scipy.sparse.csr_matrix((values, (ii, jj)), dtype='float32',\n",
    "                                   shape=(vocabulary_size, vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QpvoJI4TAFEq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:07<00:00, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 129.66832137299934 sec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = default_timer()\n",
    "texts = data['train']['text'][:100_000]\n",
    "batch_size = 10_000\n",
    "window_size = 5\n",
    "\n",
    "cooc = scipy.sparse.csr_matrix((vocabulary_size, vocabulary_size), dtype='float32')\n",
    "for batch_start in trange(0, len(texts), batch_size):\n",
    "    batch_texts = texts[batch_start: batch_start + batch_size]\n",
    "    batch_cooc = count_token_cooccurences(batch_texts, vocabulary_size, window_size)\n",
    "    cooc += batch_cooc\n",
    "    \n",
    "\n",
    "# This cell will run for a couple minutes, go get some tea!\n",
    "reference_cooc = cooc\n",
    "\n",
    "end = default_timer()\n",
    "print(f'Took {end - start} sec...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-iwaVloAFEr"
   },
   "source": [
    "__Simple parallelism with `mp.Pool`__\n",
    "\n",
    "Many standard parallel tasks, such as applying the same function to an array of inputs, can be automated by using prebuilt primitives such as Pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_cooc(texts, slice_index, batch_size, vocabulary_size, cooc):\n",
    "    texts = texts[slice_index[0]:slice_index[1]]\n",
    "    for batch_start in trange(0, len(texts), batch_size):\n",
    "        batch_texts = texts[batch_start: batch_start + batch_size]\n",
    "        batch_cooc = count_token_cooccurences(batch_texts, vocabulary_size, window_size)\n",
    "        cooc += batch_cooc\n",
    "    return cooc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-45tfQN7AFEs"
   },
   "source": [
    "__Our next step__ is to implement a parallel version of co-occurence computation using the process pool functionality.\n",
    "\n",
    "There are multiple alternatives to mp.Pool: [joblib.Parallel](https://joblib.readthedocs.io/en/latest/), [ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor), [ipyparallel](https://github.com/ipython/ipyparallel), etc. Feel free to use whichever one you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mSp_OQzxAFEs",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:50<00:00, 16.82s/it]\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.05s/it]\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.10s/it]\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 54.67169729300076 sec...\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "start = default_timer()\n",
    "\n",
    "texts = data['train']['text'][:100_000]\n",
    "batch_size = 10_000\n",
    "window_size = 5\n",
    "n_cpu = n_slices = mp.cpu_count()\n",
    "slice_size = len(texts) // n_slices\n",
    "slices_indexes = [(i * slice_size,(i + 1) * slice_size) for i in range(n_cpu)]\n",
    "cooc = scipy.sparse.csr_matrix((vocabulary_size, vocabulary_size), dtype='float32')\n",
    "\n",
    "params = []\n",
    "for slice_index in slices_indexes:\n",
    "    param = (texts, slice_index, batch_size, vocabulary_size, cooc)\n",
    "    params.append(param)\n",
    "\n",
    "with mp.Pool(processes=n_cpu) as pool:\n",
    "    ll = pool.starmap(get_ref_cooc, params)\n",
    "    \n",
    "cooc = reduce(add, ll)\n",
    "end = default_timer()\n",
    "print(f'Took {end - start} sec...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:11<00:00, 8901.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(cooc, scipy.sparse.csr_matrix)\n",
    "assert cooc.nnz == reference_cooc.nnz\n",
    "for _ in trange(100_000):\n",
    "    i, j = np.random.randint(0, vocabulary_size, size=2)\n",
    "    assert np.allclose(cooc[i, j], reference_cooc[i, j])\n",
    "\n",
    "print(\"Perfect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zanNUlMpAFEt"
   },
   "source": [
    "__Preprocess and save the full data__\n",
    "\n",
    "Finally, let's run the preprocessing code for the entire dataset and save the results for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MQLkgNtUAFEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared indexes: [[0, 450337], [450337, 900674], [900674, 1351011], [1351011, 1801348]]\n",
      "Aligned indexes: [[0, 460000], [460000, 910000], [910000, 1360000], [1360000, 1801348]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awaiting pipe_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [02:53<00:00,  3.85s/it]\n",
      "100%|██████████| 45/45 [02:54<00:00,  3.89s/it]\n",
      "100%|██████████| 45/45 [02:56<00:00,  3.92s/it]\n",
      "100%|██████████| 46/46 [02:57<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished waiting!\n",
      "Took 178.8534705300117 sec..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [16:36<00:00, 21.66s/it]\n",
      "100%|██████████| 46/46 [16:42<00:00, 21.80s/it]\n",
      "100%|██████████| 46/46 [16:46<00:00, 21.89s/it]\n",
      "100%|██████████| 46/46 [16:50<00:00, 21.98s/it]\n"
     ]
    }
   ],
   "source": [
    "texts = data['train']['text']\n",
    "vocabulary_size = 32_000\n",
    "batch_size = 10_000\n",
    "window_size = 5\n",
    "\n",
    "# YOUR CODE: compute both vocabulary and cooc on the entire training corpora and save the results\n",
    "\n",
    "token_counts = compute_token_counts_parallel(texts, vocabulary_size, batch_size)\n",
    "token_counts = Counter(token_counts)\n",
    "\n",
    "n_cpu = n_slices = mp.cpu_count()\n",
    "slice_size = len(texts) // n_slices\n",
    "slices_indexes = [(i * slice_size,(i + 1) * slice_size) for i in range(n_cpu)]\n",
    "cooc = scipy.sparse.csr_matrix((vocabulary_size, vocabulary_size), dtype='float32')\n",
    "\n",
    "params = []\n",
    "for slice_index in slices_indexes:\n",
    "    param = (texts, slice_index, batch_size, vocabulary_size, cooc)\n",
    "    params.append(param)\n",
    "\n",
    "with mp.Pool(processes=n_cpu) as pool:\n",
    "    ll = pool.starmap(get_ref_cooc, params)\n",
    "    \n",
    "cooc = reduce(add, ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CdxoorbBAFEu"
   },
   "outputs": [],
   "source": [
    "assert len(vocabulary) == vocabulary_size\n",
    "assert cooc.shape == (vocabulary_size, vocabulary_size)\n",
    "assert 440_000_000 < np.sum(cooc) < 450_000_000\n",
    "assert 0.05 < cooc.nnz / vocabulary_size ** 2 < 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GZXhjIFJAFEu"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocessed_data.pcl', 'wb') as f:\n",
    "    pickle.dump((vocabulary, cooc.tocoo()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rN314cCAFEu"
   },
   "source": [
    "### Finally, GloVe!  (20% points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vNLaUatnAFEu"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocessed_data.pcl', 'rb') as f:\n",
    "    vocabulary, cooc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e401y6JAFEu"
   },
   "source": [
    "### Weight function\n",
    "![image.png](https://i.imgur.com/Cdu6BJ5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-O7lIzauAFEw"
   },
   "outputs": [],
   "source": [
    "def compute_loss_weights(counts_ij):\n",
    "    \"\"\" Compute GloVe weights \"\"\"\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    return [(x / x_max)**alpha if x < x_max else 1 for x in counts_ij]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OTya47l8AFEw"
   },
   "outputs": [],
   "source": [
    "dummy_weights = compute_loss_weights(np.arange(0, 200, 30))\n",
    "dummy_reference_weights = [0. , 0.40536, 0.681731, 0.92402, 1. , 1. , 1.]\n",
    "assert np.allclose(dummy_weights, dummy_reference_weights, rtol=1e-4, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4a6DJqjAFEw"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "![img](https://i.imgur.com/bkEBBLk.png)\n",
    "\n",
    "\n",
    "__The goal__ is to compute the loss function as per formula above. The only difference is that you should take _mean_ over batch instead of sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bempH2-4AFEw"
   },
   "outputs": [],
   "source": [
    "def compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij):\n",
    "    \"\"\"\n",
    "    Compute GloVe loss function given embeddings, biases and targets\n",
    "    \n",
    "    :param emb_ii, emb_jj: vectors of left- and right-side words, shape: [batch_size, embedding_dimension]\n",
    "    :param bias_ii, bias_jj: biases for left- and right-side words, shape: [batch_size]\n",
    "    :param counts_ij: values from co-occurence matrix, shape: [batch_size]\n",
    "    :returns: mean GloVe loss over batch, shape: scalar\n",
    "    \"\"\"\n",
    "    weights = compute_loss_weights(counts_ij)\n",
    "    target = np.log(counts_ij)\n",
    "    loss = weights * (np.sum(emb_ii * emb_jj, axis=1) + bias_ii + bias_jj - target)**2\n",
    "    return np.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IFO8TyMkAFEx"
   },
   "outputs": [],
   "source": [
    "dummy_emb_ii = np.sin(np.linspace(0, 10, 40)).reshape(4, 10)\n",
    "dummy_emb_jj = np.cos(np.linspace(10, 20, 40)).reshape(4, 10)\n",
    "dummy_bias_ii = np.linspace(-3, 2, 4)\n",
    "dummy_bias_jj = np.linspace(4, -1, 4)\n",
    "dummy_counts_ij = np.abs(np.sin(np.linspace(1, 100, 4)) * 150)\n",
    "\n",
    "dummy_loss = compute_loss(dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij)\n",
    "assert np.shape(dummy_loss) == ()\n",
    "assert np.allclose(dummy_loss, 1.84289356)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c81syAvPAFEx"
   },
   "outputs": [],
   "source": [
    "def compute_grads(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij):\n",
    "    \"\"\"\n",
    "    Compute gradients of GloVe loss with respect to emb_ii/jj and bias_ii/jj\n",
    "    Assume the same parameter shapes as above\n",
    "    :returns: (grad_wrt_emb_ii, grad_wrt_emb_jj, grad_wrt_bias_ii, grad_wrt_bias_jj)\n",
    "    \"\"\"\n",
    "    weights = np.array(compute_loss_weights(counts_ij))\n",
    "    target = np.log(counts_ij)\n",
    "    common = weights * (np.sum(emb_ii * emb_jj, axis=1) + bias_ii + bias_jj - target) * 2 \n",
    "    grad_wrt_bias_ii = grad_wrt_bias_jj = common\n",
    "        \n",
    "    grad_wrt_emb_jj = common.reshape(-1, 1) * emb_ii\n",
    "    grad_wrt_emb_ii = common.reshape(-1, 1) * emb_jj\n",
    "    \n",
    "    # correction by division N:\n",
    "    ls = [grad_wrt_emb_ii, grad_wrt_emb_jj, grad_wrt_bias_ii, grad_wrt_bias_jj]\n",
    "    for i in range(len(ls)):\n",
    "        ls[i] = ls[i] / ls[i].shape[0]\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "WMKNKChwAFEx"
   },
   "outputs": [],
   "source": [
    "grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
    "    dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij)\n",
    "\n",
    "assert np.shape(grad_emb_ii) == np.shape(grad_emb_jj) == np.shape(dummy_emb_ii)\n",
    "assert np.shape(grad_bias_ii) == np.shape(grad_bias_jj) == np.shape(dummy_bias_ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "LBrXg99RAFEy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/db[ii] OK\n",
      "dL/db[jj] OK\n"
     ]
    }
   ],
   "source": [
    "from utils import eval_numerical_gradient\n",
    "reference_grad_bias_ii = eval_numerical_gradient(\n",
    "    lambda x: compute_loss(dummy_emb_ii, dummy_emb_jj, x, dummy_bias_jj, dummy_counts_ij),\n",
    "    x=dummy_bias_ii)\n",
    "\n",
    "assert np.allclose(reference_grad_bias_ii, grad_bias_ii, rtol=1e-4, atol=1e-3)\n",
    "print(\"dL/db[ii] OK\")\n",
    "\n",
    "reference_grad_bias_jj = eval_numerical_gradient(\n",
    "    lambda x: compute_loss(dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, x, dummy_counts_ij),\n",
    "    x=dummy_bias_jj)\n",
    "\n",
    "assert np.allclose(reference_grad_bias_jj, grad_bias_jj, rtol=1e-4, atol=1e-3)\n",
    "print(\"dL/db[jj] OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.69662399, -0.72184175, -0.85898326, -0.00747297]),\n",
       " array([-0.69662399, -0.72184175, -0.85898326, -0.00747297]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_grad_bias_jj, grad_bias_jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "v8Unp2sHAFEy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dEmb[ii] OK\n",
      "dL/dEmb[ii] OK\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "reference_grad_emb_ii = eval_numerical_gradient(\n",
    "    lambda x: compute_loss(x, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij),\n",
    "    x=dummy_emb_ii)\n",
    "\n",
    "assert np.allclose(reference_grad_emb_ii, grad_emb_ii, rtol=1e-4, atol=1e-3)\n",
    "print(\"dL/dEmb[ii] OK\")\n",
    "\n",
    "\n",
    "reference_grad_emb_jj = eval_numerical_gradient(\n",
    "    lambda x: compute_loss(dummy_emb_ii, x, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij),\n",
    "    x=dummy_emb_jj)\n",
    "\n",
    "assert np.allclose(reference_grad_emb_jj, grad_emb_jj, rtol=1e-4, atol=1e-3)\n",
    "print(\"dL/dEmb[ii] OK\")\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiRd2ERLAFEz"
   },
   "source": [
    "### Part 3: Parallel GloVe training (50% points)\n",
    "\n",
    "Finally, let's write the actual parameter server for parallel GloVe training. In order to do so efficiently, we shall use shared memory instead of pipes.\n",
    "\n",
    "You can find an example of how shared memory works below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8AF358WAFE0"
   },
   "source": [
    "### Demo: shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-X0UxoWpAFE0"
   },
   "outputs": [],
   "source": [
    "def make_shared_array(shape, dtype, fill=None, lock=True):\n",
    "    \"\"\" Create a numpy array that is shared across processes. \"\"\"\n",
    "    size = int(np.prod(shape))\n",
    "    ctype = np.ctypeslib.as_ctypes_type(dtype)\n",
    "    if lock:\n",
    "        x_mp = mp.Array(ctype, size, lock=True).get_obj()\n",
    "    else:\n",
    "        x_mp = mp.Array(ctype, size, lock=False)\n",
    "    array = np.ctypeslib.as_array(x_mp)\n",
    "    if fill is not None:\n",
    "        array[...] = fill\n",
    "    return np.reshape(array, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9Lc775zDAFE1"
   },
   "outputs": [],
   "source": [
    "shared_array = make_shared_array((5, 5), 'float32', fill=1)\n",
    "normal_array = np.ones((5, 5), 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yXishO2FAFE2"
   },
   "outputs": [],
   "source": [
    "def proc_A():\n",
    "    time.sleep(0.5)\n",
    "    print(\"A: setting value at [2, 3]\")\n",
    "    shared_array[2, 3] = 42\n",
    "    normal_array[2, 3] = 42\n",
    "    time.sleep(1)\n",
    "    print(f\"A: value after 1.5s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
    "    \n",
    "def proc_B():\n",
    "    print(f\"B: initial value: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
    "    time.sleep(1)\n",
    "    print(f\"B: value after 1s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
    "    print(\"B: dividing value at [2, 3] by 2\")\n",
    "    shared_array[2, 3] /= 2\n",
    "    normal_array[2, 3] /= 2\n",
    "    time.sleep(1)\n",
    "    print(f\"B: value after 2s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "81N-7Xi9AFE3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: initial value: normal = 1.0\t shared = 1.0\n",
      "A: setting value at [2, 3]\n",
      "B: value after 1s: normal = 1.0\t shared = 42.0\n",
      "B: dividing value at [2, 3] by 2\n",
      "A: value after 1.5s: normal = 42.0\t shared = 21.0\n",
      "B: value after 2s: normal = 0.5\t shared = 21.0\n"
     ]
    }
   ],
   "source": [
    "mp.Process(target=proc_A).start()\n",
    "mp.Process(target=proc_B).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "BWm-b7dRAFE4"
   },
   "outputs": [],
   "source": [
    "# the same can be done with individual values:\n",
    "x = mp.Value(np.ctypeslib.as_ctypes_type(np.int32))\n",
    "x.value += 1 # shared across all processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-du4ddAFE6"
   },
   "source": [
    "__So, let's put all trainable parameters in shared memory!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "c1JB__1kAFE6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SharedEmbeddings:\n",
    "    \"\"\"\n",
    "    Word embeddings trainable parameters, allocated in shared memory\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary_size: int, embedding_dimension: int, init_scale: float = 0.01):\n",
    "        self.embeddings = make_shared_array([vocabulary_size, embedding_dimension], np.float32, lock=False)\n",
    "        self.embeddings[...] = np.random.randn(*self.embeddings.shape) * init_scale\n",
    "        \n",
    "        self.biases = make_shared_array([vocabulary_size], np.float32, fill=0.0, lock=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr5PV0MfAFE6"
   },
   "source": [
    "### Training (single-core baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "max_steps = 10 ** 6\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "timestep_history = []\n",
    "loss_history = []\n",
    "\n",
    "model = SharedEmbeddings(vocabulary_size, embedding_dimension=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "kYwgvX5CAFE7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABHAElEQVR4nO29eXxcZ3X//z4z0mjXaN+9yLZsx7tjZyObgZA4gSRQAnVaIBRogJIWmtKSUJqW9Eu/UH5QKE0LKYTlCyQEsmASJyGLld2JHe+75VWSLWvfl9HMPL8/7p3RSBpJo31mfN6v17x873Ofe++RRv7cc89znvOIMQZFURQlfnHMtgGKoijK9KJCryiKEueo0CuKosQ5KvSKoihxjgq9oihKnJMw2wYMJS8vz8yfP3/C53d1dZGWljZ1Bk0DauPUEQt2xoKNEBt2xoKNMDt2vvPOO43GmPywB40xUfVZt26dmQxbt26d1Pkzgdo4dcSCnbFgozGxYWcs2GjM7NgJ7DAj6KqGbhRFUeIcFXpFUZQ4R4VeURQlzlGhVxRFiXNU6BVFUeIcFXpFUZQ4R4VeURQlzrnghP6d0y3sr22bbTMURVFmjAtO6P95836+9ezh2TZDURRlxoi6EgjTTVOnB11rRVGUC4m4Evrq5m56vKOreHOXB4fIDFmkKIoy+8RN6OZ0UxfXfHsrr9d6R+zT4/HR5/XT1tM/g5YpiqLMLnEj9PNy01hZ6qayuh8zQmymudsDQGefF4/XP5PmKYqizBoRCb2IbBSRIyJSJSL3jNLvwyJiRGR9SNu99nlHROSGqTB6JG6/dC41nYZd1a1hj7d0eYLb6tUrinKhMKbQi4gTeAC4EVgG3C4iy8L0ywC+CLwV0rYM2AQsBzYC/21fb1q4eXUJyU54+K0zYY+3dIcKvSdsH0VRlHgjEo/+UqDKGHPCGOMBHgFuDdPvX4FvAb0hbbcCjxhj+owxJ4Eq+3rTQnpSApcXJ/CHvWdp7x3usbd094fdVhRFiWciybopBapD9muAy0I7iMjFwBxjzNMi8vdDzt025NzSoTcQkTuBOwEKCwuprKyMyPhwXJrXT2WN8O1HK3nv3MRBx94+PSDur729k65Ts5N01NnZOamfcSaIBRshNuyMBRshNuyMBRsh+uyctNKJiAP4LvDJiV7DGPMg8CDA+vXrzYYNGyZuUGUlK0qdbG823P/xq5GQVMrdLxyFQ8cAKFuwhA3r50z8PpOgsrKSSf2MM0As2AixYWcs2AixYWcs2AjRZ2ckoZtaIFQRy+y2ABnACqBSRE4BlwOb7QHZsc6dFm6/dC6H6zrYUzO41EFrdz8JDgluK4qiXAhEIvTbgQoRKRcRF9bg6ubAQWNMmzEmzxgz3xgzHytUc4sxZofdb5OIJIlIOVABvD3lP8UQblldQqrLyW93VA9qb+7yUJKVgtMhtOpgrKIoFwhjhm6MMV4RuQt4DnACDxljDojI/ViL0W4e5dwDIvIocBDwAl8wxvimyPYRyUhOZEWJm2P1nYPaW7o9ZKe56OrzqkevKMoFQ0QxemPMFmDLkLb7Rui7Ycj+N4BvTNC+CVPkTmb3kHz6lm4P+elJdPQmqtArinLBEDczY4dS7E6mrr130CzZlq5+slNdZKe6NHSjKMoFQ9wKfZE7GY/XPyhfvtUO3WSlqEevKMqFQ9wKfbE7GYBzbT0A9Hl9dHl8ZKcm4k5VoVcU5cIhboW+yJ0CQF2bNVE3IOxZgdBNt4ZuFEW5MIhboR/w6C2hb7YLmuXYoZsuj08rWCqKckEQt0Kfl56E0yFBjz5Q0CwrNZGsVKs0glawVBTlQiBuhd7pEAoykoIefUuXJeo5aS7cqS4ADd8oinJBELdCD1bmTV27NRgb8Oit9ErLo29Vj15RlAuAuBb6YndyiEcfErpJCXj0KvSKosQ/cS30RZkp1LVZk6ZauvtJczlJSnAGY/QtGrpRFOUCIK6FvtidTLfHR0efNzhZChgYjFWPXlGUC4C4FvoiO8Wyrq2X5m4P2fYgbHpSglawVBTlgiGuhT40l76luz/oyYsIWSmJupygoigXBHEt9AMefQ8tXR5y7NANWOEbDd0oinIhENdCX5CRjEjAox8I3YBVCkFDN4qiXAjEtdC7EhzkpiVR3dxDR693sNCnJAYnUSmKosQzcS30YMXpD9e1A5Cdlhhsd6cmagkERVEuCOJe6IvcyRw7by0pGOrRawVLRVEuFOJe6IvdyXh8VpXKoaEbrWCpKMqFQNwLfSDzBgYmSoVu64CsoijxTtwLfXGI0A9Or7S2NcVSUZR4J+6FvigzJbg9OL0yUO9GhV5RlPgmIqEXkY0ickREqkTknjDHPyci+0Rkt4i8JiLL7Pb5ItJjt+8WkR9O9Q8wFgGPPjnRQYrLGWwfqGCpoRtFUeKbhLE6iIgTeAB4H1ADbBeRzcaYgyHdfm2M+aHd/xbgu8BG+9hxY8yaKbV6HARi9KHePITG6NWjVxQlvonEo78UqDLGnDDGeIBHgFtDOxhj2kN20wAzdSZOjuREJ9mpiSMLvXr0iqLEOWLM6JosIrcBG40xn7H3Pw5cZoy5a0i/LwB3Ay7gPcaYYyIyHzgAHAXaga8ZY14Nc487gTsBCgsL1z3yyCMT/oE6OztJT08f1PbPb/SQ4RK+vH5gYNYYw6f/2M1N5Yncttg19DLTSjgbo41YsBFiw85YsBFiw85YsBFmx853v/vd7xhj1oc9aIwZ9QPcBvw4ZP/jwH+N0v/PgJ/b20lArr29DqgGMke737p168xk2Lp167C2/bWt5mhd+7D2i+//o7n38b2Tut9ECGdjtBELNhoTG3bGgo3GxIadsWCjMbNjJ7DDjKCrY8bogVpgTsh+md02Eo8A/2M/RPqAPnv7HRE5DiwGdkRw3yljeYk7bHt2movmTg3dKIoS30QSo98OVIhIuYi4gE3A5tAOIlIRsvt+4Jjdnm8P5iIiC4AK4MRUGD4VzMtJ5WRj12yboSiKMq2M6dEbY7wichfwHOAEHjLGHBCR+7FeFTYDd4nIdUA/0ALcYZ9+DXC/iPQDfuBzxpjm6fhBJsKignRePdaI1+cnwRn3UwoURblAiSR0gzFmC7BlSNt9IdtfHOG8x4DHJmPgdLKwIB2Pz091Sw/leWmzbY6iKMq0cEG7sRUF1qj4sfMds2yJoijK9HFBC/1CW+irGjpn2RJFUZTp44IW+szkRIoyk6k6r0KvKEr8ckELPVgDsurRK4oSz6jQF6RTVd8ZmOylKIoSd6jQF6TT7fFxtq13tk1RFEWZFlToAwOy9Rq+URQlPrnghb5ChV5RlDjnghf63PQkslMTqarXXHpFUeKTC17oASoKMtSjVxQlblGhx5o4dUwzbxRFiVNU6LEGZFu7+2nq0pLFiqLEHyr06ICsoijxjQo9AymWx1ToFUWJQ1TogWJ3MmkuJ8dV6BVFiUNU6AERCZZCUBRFiTdU6G3m5KRS09I922YoiqJMOSr0NqXZKZxt7cXv1xRLRVHiCxV6m7KsFDw+P42dfbNtiqIoypSiQm9Tmp0CQE1rzyxboiiKMrWo0NuUZqUCUNOiQq8oSnyhQm8T8OhrVegVRYkzIhJ6EdkoIkdEpEpE7glz/HMisk9EdovIayKyLOTYvfZ5R0Tkhqk0fipJT0rAnZJIbatm3iiKEl+MKfQi4gQeAG4ElgG3hwq5za+NMSuNMWuAfwe+a5+7DNgELAc2Av9tXy8qKc1KUY9eUZS4IxKP/lKgyhhzwhjjAR4Bbg3tYIxpD9lNAwI5ircCjxhj+owxJ4Eq+3pRSWl2CrU6GKsoSpyREEGfUqA6ZL8GuGxoJxH5AnA34ALeE3LutiHnloY5907gToDCwkIqKysjMCs8nZ2dEz+/q48zjV62bt2KiEzYhrGYlI0zRCzYCLFhZyzYCLFhZyzYCNFnZyRCHxHGmAeAB0Tkz4CvAXeM49wHgQcB1q9fbzZs2DBhOyorK5no+VXOEzx/+hBrL7uSrFTXhG0Yi8nYOFPEgo0QG3bGgo0QG3bGgo0QfXZGErqpBeaE7JfZbSPxCPDBCZ47q5QFcuk1Tq8oShwRidBvBypEpFxEXFiDq5tDO4hIRcju+4Fj9vZmYJOIJIlIOVABvD15s6eHQC69xukVRYknxgzdGGO8InIX8BzgBB4yxhwQkfuBHcaYzcBdInId0A+0YIdt7H6PAgcBL/AFY4xvmn6WSaO59IqixCMRxeiNMVuALUPa7gvZ/uIo534D+MZEDZxJslMTSUl0qkevKEpcoTNjQxARSrNTtFyxoihxhQr9EEqzNJdeUZT4QoV+CKXZOjtWUZT4QoV+CKVZKbR099Pt8c62KYqiKFOCCv0QyjTzRlGUOEOFfghlugCJoihxhgr9EIKTpsbh0ff2+3h0e7WuN6soSlSiQj+EgowkEp0yrsyb5w7U8Q+P7WVXdev0GaYoijJBVOiH4HAIxe7xZd6caOgC0Px7RVGiEhX6MJRmjW/S1KmmgNBrXF9RlOhDhT4M8/PSONHYhTGRxdxPNlpCrxOtFEWJRlTow1BRkE5rdz9NXZ4x+xpjBoRePXpFUaIQFfowLCpIB6CqvnPMvk1dHjp6rclV6tErihKNqNCHoaLQEvpjEQj9KdubX5ifRk1Ld8ThHkVRlJlChT4MRZnJpCclcDwCoQ+Eba6uyKe3309zBOEeRVGUmUSFPgwiwsL8NI7Vd4zZ92RjFwkO4dLyHEDDN4qiRB8q9COwqCAjohj9qaYu5uakMjdn/DNqFUVRZgIV+hFYVJDO+fY+2nv7R+13oqGL+XlpA8XQ1KNXFCXKUKEfgYoIMm/8fsPppm7K89JwpySS5nLqpClFUaIOFfoRCKZYnh9Z6M939NLT72N+XhoiQll2qnr0iqJEHSr0IzAnJxVXgoOqhpGFPpBxsyAvDcBeb1aFXlGU6CIioReRjSJyRESqROSeMMfvFpGDIrJXRF4UkXkhx3wistv+bJ5K46cTp0NYkJfGsfMjZ96carTq4cwPCH1WCrVa2ExRlChjTKEXESfwAHAjsAy4XUSWDem2C1hvjFkF/A7495BjPcaYNfbnlimye0aoKMwYw6PvJCnBQXFmMmB59O29XjrGGMBVFEWZSSLx6C8FqowxJ4wxHuAR4NbQDsaYrcaYgCu7DSibWjNnh0X56dS09NDj8YU9frKxm/m5aTgcAlgePWjmjaIo0UUkQl8KVIfs19htI/Fp4JmQ/WQR2SEi20Tkg+M3cfZYVJCOMXB8BK/+ZGMn8/NSg/ulut6soihRSMJUXkxEPgasB64NaZ5njKkVkQXASyKyzxhzfMh5dwJ3AhQWFlJZWTlhGzo7Oyd1fiitHX4ANr+8ncaSwb8qvzGcauxmSboneL/WXqv/1rf34jyfOCM2ThexYCPEhp2xYCPEhp2xYCNEoZ3GmFE/wBXAcyH79wL3hul3HXAIKBjlWj8DbhvtfuvWrTOTYevWrZM6P5S+fp9ZcO/T5tvPHh527HRjl5n3lafMI2+fDrb5fH5T8dUt5htPH5wxG6eLWLDRmNiwMxZsNCY27IwFG42ZHTuBHWYEXY0kdLMdqBCRchFxAZuAQdkzIrIW+BFwizGmPqQ9W0SS7O084Erg4ISfSjOMK8HBvNzUsJOmTtqrSpXnpQfbHA6hNHt8yxAqiqJMN2OGbowxXhG5C3gOcAIPGWMOiMj9WE+QzcC3gXTgtyICcMZYGTYXAT8SET/WeMA3jTExI/RgDciGK24WKE8cGqMHexlCHYxVFCWKiChGb4zZAmwZ0nZfyPZ1I5z3BrByMgbONosLM3jxcD29/T6SE53B9uMNnaQnJZCfnjSof2lWCi8erh96GUVRlFlDZ8aOwYpSNz6/4dC59kHte2vaWF6Sif0GE6Q0O4XGzj56+8OnZCqKosw0KvRjsKrMDcC+2rZgW7/Pz8Fz7cFjoQSqWFY36wxZRVGiAxX6MSh2J5OX7mJvzYDQH6nrwOP1s7Isa1j/ZSWZABw42z7smKIoymygQj8GIsKKUjf7QoQ+4N2vDuPRL8pPJznRMejBoCiKMpuo0EfAqlI3x+o76PZ4Adhb04o7JTG4qlQoCU4Hy4oz2VfbOsNWKoqihEeFPgJWlmXhNwQHZPfWtLGqzD1sIDbAqrIs9te24/ObmTRTURQlLCr0ERAYdN1b00Zvv48jdR2sLB0etgmwstRNT79vxBo5iqIoM4kKfQQUZiZTkJHEvpo2Dp1rx+s3rAozEBsgmKkTYZz+wVeOc1AHbxVFmSZU6CNkZambvbVtwUHWcKmVARbkp5Pqcg5KyRyJ5i4P/7blMI/trJkyWxVFUUJRoY+QlWVujjd08ubxJvLSkyh2J4/Y1+kQlpdksremdczrBuL+LV2eqTJVURRlECr0EbKqzI0x8MKh86MOxAZYWZrFwXPteH3+UfsdOGt5/c3dKvSKokwPKvQRssIefLXi8yOHbQKsKnPT2+8fdSlCIBibb1aPXlGUaUKFPkIKMpKD4ZpIhH5lSKbOaBw8p0KvKMr0okI/DgJe/crSrDH7luemkZ6UMGrmTW+/j+MNXYio0CuKMn1M6VKC8c5H1pWRnZpIfkbSmH0dgQHZUTJvjp7vwOc3rCx1s6+2bVgpZEVRlKlAPfpxcP3yIv79ttUR919V5ubQuXb6RxiQDRQ+u6oiD1CvXlGU6UGFfhpZWZaFx+tn/whe/cGz7WQkJQSLo6nQK4oyHajQTyPvWpiLOyWRrz6xnx7P8IVIDp5r56LiTHLtVapU6BVFmQ5U6KeRvPQkvrdpDYfr2vnqE/uwFmq38NurVi0rySQnzQVAi+bSK4oyDajQTzPvXlLA3163mCd21fKLN08H2081ddHt8bGsOJOcVEvomzpV6BVFmXo062YGuOvdi9hb08q/PnWQIncySQzkzy8rycSdkohDRvfo/X6DCGPOyFUURRmKevQzgMMhfPdP17CyzM3nf/kOL9f0c/BsOwkOoaIwHYdDyE510TRKjP5vHtnFXQ/vmkGrFUWJFyISehHZKCJHRKRKRO4Jc/xuETkoIntF5EURmRdy7A4ROWZ/7phK42OJzOREfvWZy7iqIp+f7vfw8NtnWFSQTlKClTefk+YasbBZfUcvW/adG7NI2qnGLurbe6fadEVRYpwxhV5EnMADwI3AMuB2EVk2pNsuYL0xZhXwO+Df7XNzgH8GLgMuBf5ZRLKnzvzYItWVwI8/sZ7Li520dPezrDgzeCw7bWSPfvPus/gN1LX14h9l1arP/fIdvv7UwSm3W1GU2CYSj/5SoMoYc8IY4wEeAW4N7WCM2WqM6bZ3twFl9vYNwPPGmGZjTAvwPLBxakyPTVwJDu5clcQ3/2QlX3jPomB77ige/RO7agHo9xkaO/vC9jHGcKqpizNN3WGPT5S2nn42Pfgmx853TOl1FUWZOSIZjC0FqkP2a7A89JH4NPDMKOeWDj1BRO4E7gQoLCyksrIyArPC09nZOanzZ4Luri6K5ATVB04Efzk9bX3UtXqH2V7b4efA2R6W5zo40OTnDy+9zsKs4WUSOjyG3n4/pxvaI/75W/v8dHmgNGP48z7we9x53su2E338ZMs2NpYnjvMnnX5i4fuOBRshNuyMBRsh+uyc0qwbEfkYsB64djznGWMeBB4EWL9+vdmwYcOEbaisrGQy588E4Wx8x3OEV2qquOaaa3E4BjJrvvnMYZyOE9zzwfV8/CdvU7xwGRtWFg+75v7aNnjpNdo9hndddQ2uhLFf1u7+zW5eOHSe7V+7LjhWMNTG7c8dBo4j7kI2bFg1oZ93OonV7zsaiQU7Y8FGiD47Iwnd1AJzQvbL7LZBiMh1wD8Ctxhj+sZzrgLZqS78xgqVBPD7Db/fXcu1i/ODi5Gfbe0Je35tSPv5CAdkD9V10N7r5ZWjjSP22V3dCsDx+q6IrqkoSvQRidBvBypEpFxEXMAmYHNoBxFZC/wIS+TrQw49B1wvItn2IOz1dpsyhNx0a9JU6EpT2040ca6tlw+tLcWdkkhKopOzreFFPPQBcK5tbKH3+Q3H7UVR/rDnbNg+fr9hb7VVp+f4GAuoKIoSvYwp9MYYL3AXlkAfAh41xhwQkftF5Ba727eBdOC3IrJbRDbb5zYD/4r1sNgO3G+3KUPItmfHhta7eWJXLelJCbxvWSEiQklWMufawnv0g4U+fJ9Qalq68Xj9ZKUm8sKh82Fr8Zxo7KSjz8uSwgyaujy6rq2ixCgR5dEbY7YYYxYbYxYaY75ht91njAkI+nXGmEJjzBr7c0vIuQ8ZYxbZn59Oz48R+wTq3YQK/RvHm7h2cX6wRn1JVsqIoZuzrb0U2HXy64Z49N0eLx29/YPajp23PPS/vHoB3R4fLx2uZyi7bW/+w+us8XP16hUlNtGZsVHCUKFv6OijtrWHtXOzgn2K3cmcHSEsU9vaw5KiDNKTEoaFbv7u0T185uc7BrUdq7dE+88vm0t+RlLY8M2e6lb7jaIIUKFXlFhFhT5KGCr0gVmwq8qygn1KslJo6Oijzzs8zHK2tYcSdwrF7uRhHv3emjZ2nG6h2+MNtlXVd1KYmURWqov3ryzmpSP1w7z+3dWtrCpzMzcnFVeCg+MNOiCrKLGICn2UkJzoJNXlDAr9npo2HAIrSgdmz5a4UwA43zZ40lSf10d9Rx8lWSkUuZM5F5J109vv42xbDz6/YfeZ1mB7VX0HFQUZANy8ugSP18/zB88Hj3t8Vhnl1XOycDqEBXlpHK9Xj15RYhEV+igiO3Vgduye6lYWF2aQ6hqY6lCSZQn92SGDrQHhL8lKtj36geMnG7sIlMHffqoFsGbRVtV3sqggHYCL52ZRmpUSnIELcKbdj9dvWG2/USzMTx936Karz8v1//Eyb5/U8XdFmU1U6KOI3HSr3o0xhr01rUGRDVCclQwMz6UP5NCXZqVQ5E6hvqMvuE7tCTvckupysuO0Jbjn2nrp8viCQi8ifOzyebx6rJGn956zzmuzzg+MESzMT+NMc3fYsNFInGjo4uj5Tl4+OnygV1GUmUOFPorITnXR3OWhurmHlu5+Vs1xDzoeCN0MHWwNCH9JlhWjNwbqOywv/4Tthd+0spidp1vw+vzBgdgKW+gB/vLqclaXufnak/uo7+jlRJuPosxkCjOth8vCgnT8Bk41Rl5LJ/AAOlKnIR9FmU1U6KOI3DRL6PfYA7FDPfoUl5Ps1MRhHn1gv8idTJHbEuZA+OZEYxcl7mSuWZxPl8fH4bqOYIGyRSFCn+B08J2PrqHb4+Pex/Zxos3PmjkD91+Yb/UdT/gmkM9/rF4LoinKbKJCH0VkB4S+upWkBAdLijKG9Sl2D8+lP9vWS156EsmJzmFe/4mGThbkp3PJfKs69Nsnmzne0ElOmiu4KHmARQXpfGXjUl48XE99t2F1iNAvyE8DGNeAbMDOM83dYSdkKYoyM6jQRxE5aS56+n28dbKZ5SWZJDqHfz0lWSlhQzcldvx+wKPvxRjDiYYuFuSnUexOoTQrhR2nmzl2vnOQNx/KJ981nysW5AKwOiR0lOpKoDQrZVwefSDn3xjNwVeU2USFPooI5NLvq20b5E2HUpKVPKiAGQzk0ANkJieQ6nJyrq2Xhs4+Ovq8LMizvPFL5mez/VQLx+pHFnqHQ/jepjXcujCRS+bnDDq2ID9tXLn0Z1t7KLJj/Ee1nr2izBoq9FFEQOhheHw+QLE7hY5eL5191uQnY4zt0VtCLyJWLn1bTzDjptyOr6+fn0NDRx9tPf2DBmKHUpiZzIcqXMPeKAIplsaMvMpVKGdbe7hiYS4up4MjMSz0Jxu7ePjtM7NthqJMGBX6KGKQ0I/i0QOcs7369h4vXR5fsB2sUgnn2nqDQj/g0Q946IHJUuNhYUE63R7fsNDR4bp21v+fFwaNHfT7/NR39DEnJ5UF+WnB2jqxyK+2nebex/cFSzYrSqyhQh9FBIQ+MzmB+bmpYfsEPPdA+CY0hz5AsTuFurZeTjR0kpTgCB6rKEjHnWKtEjVS6GY0FtoDsieGhG8Onm2nsbNv0OLl1hgBlLiTqSjMiOnQTSBV9cFXjs+yJYoyMVToo4gcu1Tx6jlZiEjYPgGhD3jVoTn0AYrdydR39HGsvpPyvLTgilUOh7B+XjYZSQkUZiYxXgI59Q2dgz36QNmGE40DD4CAfSVZKSwuSKempYeuPi+xSH2H9bM8u7+O001a70eJPVToowh3SiLulEQut7NewlGYkYRDBgQ+UA4hVOiL3Mn4/IZ3TrcE0yIDfOXGpfzn7WtHfJCMRq79xtHUObgufUDoT4UI/cADKJnFdprosRitldPQ0cf6edkkOBz8+NWTs22OooybKV0zVpkcDofwwt3XkpU68iLcCU4HBRnJHDrXgd9vqG3twZXgCIowWB49QGeflwV5g0M0iwszWFw4/vg8QGZyIk6HDKqZD9Bir4p1MlTo7QdQsTsFp8PyJ46e7xg0CStWqO/o4+qKfBbkp/Hbd6r52/ctHjSeoijRjnr0UUZ+RlLY/PlQ3r20gBcOnefDP3yD7SebKXEnD1pQvChzwLsf6tFPBodDgmUaQgnsnxzi0btTEklLSmBuTipJCY7gjNxYorffR0evl/yMJO68ZgG9/X5+8eap2TZLUcaFCn0M8m8fWsF3PrKaM03d7DzTOihsAwzKwFmQP/5B19HITbMKr4XS0mXVsW/s9NBu17Q/19obtMvpEBbmp3M0BjNvGuyB2PyMJBYVZHDdRQX84s3T+PyRpZgqSjSgQh+DiAgfXlfGS3+3gc9vWMgd75o/6Lg7JZHkROurLc+bOo8erMygoR59U1df8H6BOH1taw8l7oEHzuLC9Jj06AMDsfn2Mo3vvaiQ5i7PiEs6Kko0okIfw7hTE/nKxqXcsLxoULuIUOxOIS/dFUynnCpy0ocLfUt3f3AlrED45lxb76A3jcVFGZxt6w16/LFCwKMPrMc7P9d6cJ7S7BslhlChj1OWFGaMOLt2MuSluWjqHFjhyuc3tHZ7WDsnCxFL6Lv6vLT19Afr5wMstidoxdrEqfqg0Fs/S+ANKTTDSFGiHc26iVO+t2kNEVYqGBc5aUm093rp9/lJdDpo7+nHb6yUzhJ3Cicbu4LliUMncQUyfY6e72DdvOypN2yaqG/vwyEDk9kKM5NISXRychx1+RVltonIoxeRjSJyRESqROSeMMevEZGdIuIVkduGHPOJyG77s3mqDFdGJznRSYrLOeXXzUm3BC+w5GFgYDYnzcWC/DRONXZR22rFtYvdA0Jflp1CbpqL7TG2rGBDRx956Uk47awmEWFebqqGbpSYYkyPXkScwAPA+4AaYLuIbDbGHAzpdgb4JPDlMJfoMcasmbypSjQQnDTV5aEgMzmYQ5+d6qI8L40ndtUOmiwVwOEQ3rUoj9eqGjHGRDRh60cvH2fL/jowBr+B5SWZfPPDq6bhpxqZ+o7e4EBsgAX5aRw+F3sDy8qFSyQe/aVAlTHmhDHGAzwC3BrawRhzyhizF/BPg41KFBEIYQQGZJtDPPr5uWl09HrZX9uGyEDJhABXLcoNlmYYC4/Xzw9eqqK120N2mot+n59HtldT39475rlTSX1HX3AgNsD8XGv9XK9P/9yV2CCSGH0pUB2yXwNcNo57JIvIDsALfNMY8+TQDiJyJ3AnQGFhIZWVleO4/GA6Ozsndf5MEMs21nZa4vbq9t301ySwrdrKojmy9x067WMv7KsmyyW8/uorg85N6LGO/+yZbVw/f/RsoAONPjr7vHx6uZO1Bd2cbPPx9Tp46OnXuLx44M92un+XtU3d5Dq6B92jr7Efr9/w2LOVFKaN7SvFwvcNsWFnLNgI0WfnTAzGzjPG1IrIAuAlEdlnjBlUBtAY8yDwIMD69evNhg0bJnyzyspKJnP+TBDLNjZ19vGPr71A0dyFbLiynIOVVXDgCDe991rqO3r57juVnO82rJ2bxYYNVw47/78OVFJHGhs2XDL6/TcfICnhDJ/74LtJcTm5yufnuzufpz25kA0bVo5p51Tg8xvan9vCqor5bNiwJNiedqqZn+x/k4JFK9iwpGDM68TC9w2xYWcs2AjRZ2ckoZtaYE7IfpndFhHGmFr73xNAJbB2HPYpUUZWqguRkNBNp4cUe+C3NCuFBHvQcuhs3QBXLspl24km+kcJexhjeOHQea5alBccUE5wOrikPIdtJ5pGte/QufYpC6k0d3nwGyjIHB66AU2xVGKHSIR+O1AhIuUi4gI2ARFlz4hItogk2dt5wJXAwdHPUqIZp13vJpBt09ztCcbtE5wO5tp19ENnxYZy1aJ8uj2+URfxOHq+k5qWHq5bVjio/fIFOZxo6OL8CHH6/bVt3Pj9V3no9ampMBmcFTtkEfW8dBfpSQkXrNDvONXMr9/SFbdiiTGF3hjjBe4CngMOAY8aYw6IyP0icguAiFwiIjXAR4AficgB+/SLgB0isgfYihWjV6GPcULLILR0echOG4i3l9ve7kge/RULc3EIvHqsccTrv3DoPADvXTo4LBIo3zySVx8Q+Iffro54ucNQhp4TnCw1xKMXEebnpXKyaXpy6R/YWsW/bD4wdsdZ4sevnuSffr+fxpCJc0p0E1EevTFmizFmsTFmoTHmG3bbfcaYzfb2dmNMmTEmzRiTa4xZbre/YYxZaYxZbf/7k+n7UZSZIict1KPvJzt1oGRvYOZoaA59KO6URFaVZfF61ehCv7rMTcGQrJ3lJW4ykhLYdmJ4Ln5DRx9P7TnHnBxr0tbb48zXN8bwkR++yT89uX/QNWFgVmwo5Xnpgzz6U41dfOFXO+mYghIPT+yq5el95yZ9neniZGMXPr/hmf11s22KEiFaAkEZN7lDPPrQWvjldlnk0hE8eoCrFuWxu7o1bN2bho4+dle38t6LCocdczqES8tzeCuMR//rt87g8fn54cfWkZGUwG+2Vw/rMxqVRxvYcbqFlw7XD7IFGJZHD1Cem0pNSzcerzUe8IOXqnh637lB50+Ebo+X4w2dNHT00dvvm9S1uvq8vHK0YVLXGIrfb4KTxZ7ac3ZKrx2veH1+vvPHI1Q3z95sahV6ZdyEhm6au6w89wAfWFXCvTcuZXlJ5ojnX7koD5/f8FwYj3Dr4XqMgevCCD1Y4ZsTjYPj9B6vn1++dZoNS/JZXuLm1rUlPL3vHG09kXvX/7PVSgSrbe0JlnBo6OgjIzmB5MThM4zn56XhN1Dd0k19Ry9/sEVv6ySF/tC5jmDpipqWyQnDPzy2l0889PaUCkxdey99Xj8l7mTePtU84niJMsAz++uCjsBsoUKvjJvcNBct3R56+61c95yQ0I07JZHPXrtw0EIoQ1k3L5sF+Wn8/e/28uXf7qGpsw+vz88LB8/zk9dOUuJO5qLi8KtghYvTb9l3joaOPj5pl2vedMlc+rx+Nu+OLDlsx6lm3j7VzIfWlgKw83QrYA3GDp0sFWC+HaI62dDFL7dZbxPr5mXzyrFG/JOoVX/gbFtwu7p54qWQ/7DnLE/vtYTl4Ln2CV9nKIFw1WevXYgxBO+hhMcYw49fs8aOaltmr7S1Cr0ybnLSXBgDJxqs//TZ41xWz5Xg4Km/vorPb1jIk7tqec93XuZd33yJz/xiB01dHu656aIRSyQsK8kkIzkhKPTGGH76xikW5KdxTUU+ACtK3SwvyeSRCMM3/115nOzURL5+63KSEx28c7oFsAqahQvbwMCg85HzHfxq22neu7SAT1wxj+YuD3tr28KeEwn7a9twJVj/LatH8ejbuvuDdg6lvqOXf/r9/uBb1ZG6qSvXcNIO27z3ogKWFmXw1N7YD9/4/IYdp6anBtPOMy3ssTPMamdxDQMVemXc5NjphlUNVimDiayfmupK4Csbl/Lsl67m0vIcVpW5+dHH1/Hmve/hltUlI57ndAiXlefwxK5aLvu3F/jMH7vZU93KHVfMH/QWsemSORw4287+MKL71N6z/HLbaeraejl0rp2XDtfzF1eWk5lsDRS/c8YS0IbOvrADsWA93Nwpifz09ZM0dXn41FXlXFORj0NGDt90e7xs/N4rPDvKIOaBs+1cVp5DUoJj1JDLt547zEd/9OagktFgPfi++vh+ejw+vr9pLXNzUscl9Ifr2jk0yhvAqcYuXAkOStwp3Ly6hJ1nWicdYpptHt1RzW0/fDPs2M9k+clrJ8lMTuCqRXljevSB8Z7pQIVeGTeBwdcqu2ZNaNbNeFlUkMH/fmI9P77jEm5YXjTmerkAn7rSEtVrF+dzw/xE/u1DK/nzy+YO6nPLmlKSEhzDBmVbuz387W9287Un93P5/32RTQ9uI83l5I4r5gNWWOlAbRu9/b5RPXqwMowaOz0sLcrgXQtzyU5zsXpOFpUjDIC+dLiew3Ud/HdlVdjjfV4fR893sKLUTVl2yoihG6/Pz7P76/D5DS8eGvxQeXrfOV44dJ6/v2EJiwrSWVKUweG6yEM3X3pkN/c8tnfE4ycbu5mXk4rDIdy8ynogB8I3fr+Jyfo/T+6yQnx/mOK3k+rmbp7dX8ftl81lUUE6ta09o6b9/vXDO/nTH705pTYEUKFXxk3Agz9uC31u+sSFfiK8a1EeD35iPf9+22o+usTFn102l4QhDwh3SiIbVxTx+921g7JXntp7jn6f4YE/u5ivbFzKkqIM7r5+Ce5Uay7AurnZeP2GN4430tPvGzFGDwOppJ+6sjwYatqwuIC9Na3DPG2wxhIA9ta0BV/nQzl2vpN+n2FFiZs5Oakjhm62nWimucuDQ+C5A4PfDv7fm6eZn5vKX1xZDsDSogxONXVHlMHT7jEcruvg6PnOEccZTjV1Bccn5uamsqrMzX+8cJTVX/8ji/5xC+v+zwsxlV9f19bL26eaSXQKz+yrm9IH1c/fOIWIcMcV8ynLTqGzz0t7j3fE/gfOto/qWEwGFXpl3AQ8+mP1VkhgMh79dPKRdXNo7/Xyx4Png22P76xhcWE6N60s4vMbFvLoZ6/g01eVB49fbC+KEgivDJ0sFcoVC3OpKEjnljUDoaYNS/IxBl45Ntir7/Z4eelwPR9aW0qqy8kvt50edr3AQOzykkzmZKeOGLp5et9Z0lxONl06l1erGunss8Sjurmbt0428+GLy4L185cUZeDzm+Db12gcabYeBj39vrDxZJ/fcKape9A6xF/ZuJSbVhTzwTUlfPqqctp6+nl8Z82Y95oK9te2TfpeT+09izHwpesW09Tl4a0pWi+hu9/wyPZqblpZTElWSnACYU1r+O+0tdtDTUsPK0rdU3L/oajQK+MmMPgaWB82K3Vq16WdKt61MJfSrBR+u8MK35xq7GLnmVb+5OKyEQd7c9JcLMhL43n74ZCfHj5GD/DR9XN4/u5rB6Vfrix1k5vmovLIYKF/6XA9vf1+Prp+DreuKeUPe8/S1j04/XN/bTsZSQnMzUmlLDuF9l7vsBTRQNjmvRcVcsvqEjxePy/b9wqEID5oZw+B5dFDZAOyh5oGvP5wD4azrT14fP5BQn/lojy++6dr+PqtK/jH9y9j/bxsHtk+sZnJoZxv76XbM7L36/H6uevXO/mH3+2NeL7BK0cb+JfNBwbVWfrDnrOsKM3kU1eWk+py8tQUZRE9dsxDl8fLZ69ZAAzMKxkpTn/grBVeGy0teTKo0CvjJtHpIDM5gX6fITM5IaK4+mzgcAgfXlfGa1WN1Lb28PiuWkTgg2tKRz3v4nnZtNgiPJpHP9I9r12czytHG/CFhD+27DtHXnoSl5bn8LHL59Lb7+d3Q7zRA2fbWFaSicMhzMmxagYN9erfPNFES3c/N60s5pL5OeSkuXjuQB3GGB7fVcvlC3KC54JVgM2V4ODI+QiEvtnH2rlZwMDbWiiBiVKBom7h+NNL5nCiYfwzk0Px+w03/+A1vvXM4RH7/HLbaU41deP1G/bWjJ3ldLyhk8//8h1+9sYpfvDiMcB68O+paePmVSWkuJxcd1Ehz+4/N2rBvUjYdaaFl854ueOK+UEPvTTbFvoRMm8G3ubUo1eiiFw782YiGTczyUfWlWEMPPZODU/squGqRXkUjVBwLUDomrZDC5pFwrVL8mnp7g+mHvZ5DS8drufGFUU4HcLyEjcXz83iV9tOBz1fn99w8Fx78D/6nGxLrGuGeIBb9p0jzeVkw5J8nA7huosK2Hq4nrdPNnOysYs/ubhsUP8Ep4NF+ekcHsOjr2/v5VyXYePyIvIzksIu4h7IoQ/16Ify/lXFE5qZHMrJpi7qO/p4c4QsmNZuD99/8Rhr5mQBVgrjaHR7vPzVL3fiSnBw/bJC/mtrFW+fbA5OcvuAneX1/lXFtHT38+bxiWff9Pv83Pv4PrKShL+7fnGwPTfNRXKiY0SPfn9tO6VZKdP2/0mFXpkQgT/I8ebQzzRzclK5YkEuP3r5ONXNPcFJUaMREHqX0zGhsNQNy4tYOzeLv3t0D388UMeeRh+9/X5uWlkc7POxy+dxorErOH5woqGT3n4/K0ozbbvtmG7IgGxo2CYQLrpheREdfV7+8cn9JCc6uHFF0TB7lhZlcGSMzJuAqAbGHcKtAnaysZuURCeFo7zlpLoSBmYm229Fe6pb+dqT+0YNxYSy60wrAMfqO8PObv7BS1V09PbzzQ+vpDwvbcT5BGClm37tif0cre/g+5vW8t0/XcPcnFS+9MguHt9VyyXzs4NhlWsX55OelDCpSWA/ee0kh+s6+NgyFxnJA387IkJJVsqIHv1++21uulChVyZEQOhzo1zoAT6yvowuj49Ul5Mblg8XwqEsyk8nIzmB/IykiNa2HUpyopOff+pSVpS6+cKvd/L7Kk8wbBPg/auKWVqUwZd/u4dj5zuCMdrAq747JZGMpIRBoZtA2Ob9qwYeGFcuyiPN5aSqvpMblhcNEpcAS4oyON/eR6u9vm843jzeREqCFTqoKEinqr5zWJz9VFMX83JTx/ydBGYmP7m7lq2H69n04DZ+ue0MT+2JTEB32R66MQwrZ32+y88v3jzFR9fPYWlRJmvnZrHzdMuIYwK/eusMj++q5YvvreAaW8i/v2kt9R19nGzsGjRnIznRyfuWFfLsgboJ5bQ3d3n43gtHuX5ZIesKh6/pVDqC0Hf1eTnZ2MWKaQrbgAq9MkHy7JTKaM24CeXGFcW4UxJ5/8pi0pLGXlTN4RCurshjYUH6hO+ZmZzILz59KctK3NR2GjauKAxmwgAkJTj5yScvISnByV/8bDsvH20gKcHBAjssIiKU5aRSHfKq/9QeK2xz7eL8YFtyojO4ytXQsE2AJfaA7GjhmzdPNLE0x4nTISwqzKCzz0vdkDo2pxq7Rg3bBFhR6mZFaSY/eKmKz/xiBwsL0pibk8pjQ8Yk2nr6+Yff7RlWL2fXmVbWzMnCIbBziLf+RJWHRKeDu+2wyLp52TR1eTgTJkPp5aMN/PPmA7x7ST5/856KYPvqOVl8ZeNSMpISBr1lAdy4ooi2nn52nB7/GMPLR60B97vesyjs8bLslLChm0Pn2jGG4NvcdKBCr0yIgEcf7TF6gBSXky1fvJp/uWV5xOd896Nr+NHH1k3qvpnJifziU5dyw/wEPnvNwmHHS7NS+Mkd62ns7OOJXbVcVJw5aD7AnOyUoEff5/XxzP5z3LC8aFiRtc9cXc6frp/DlQtzw9qxtGj0Ugi1rT2cbupmaY513Qr7AReaeeP1+TnT3B3MoR+L2y+dS2NnH+9amMsjd17BR9eX8dbJ5kFvKD99/SSP7qgZtIhJt8fL4bp2rq7IY0lR5qD4e2efl3fO+/jIurLgjOVAmG1o+Obg2Xa+8KudLC7M4Ad/dvGw2kt/ec0Cdt73vuBYU4DAW1cgfBTKubaeUfPsXz7SQG6aa0TPvDQrhaYuz7AsocDs7ekaiAUVemWC5KRZ/0GiPUYfoDQrJSJvPkCyvTziZHGnJHL70qRBmTChrJ6TxX98dA0Aq8oG/0efk5NKTYs1m/KVo42093q5ec3w8hBr52bzrdtWDZs0FqAwMwl3SuKIHn1g8PGiHOv8gNCHDsjWtvbg9ZtgjZ+x2HTJXB765Hp+csclpCclcKud6RRIAe32ePnZG6cAazZvIPSyr6YNv4G1c7NYNy+L3Wdag9lLLxw8T78fbg4Jt1QUZJCRlDBI6OvaevnUz7aTnpTATz9p3T8c4bLFslJdlOelDQsZtXR52PDtSh5+O/zKWn6/4ZVjjVyzOH/Egn4jZd4cONtOXrpr1LGPyaJCr0yIQGw+JwZCN9HOjSuL+c2dl/PXIeEFsDz6nn4fjZ0eNu85S3ZqIlctyhv39UWEJaMMyL55vIns1ETKMiw5yE1PIifNNWhANjBnIlKP3ukQ3rO0MFigbU5OKpeV5/D4rlqMMfxmezWt3f18aG0pVfWdwfTPXbbArpmTzcVzs+no8wZTPZ/ae47sJOHiudmD7rNmbhY7bQ/cGMPf/mY3Hb39PPTJS8bMsArHmjlZ7K5uHRT331XdQp/XP+KEqv1n22ju8gwKqw2lNMt62A8N3+w/a2VbTWQ8KFJU6JUJEStZN7HCZQtyh01/D7wFHD3fwfMH67hpZfGE5ywsLcrg6PnhA6ydfV5ePdbAZeW5OEKEZlFBOlUhufSngkIf/s0kEj58cRknG7vYcbqFH796kkvmZ/PVmy7CIQP1cnadaWF+bio5aa5gWGbn6Vbaevp55WgDlxY5h3nMF8/N5khdOx29/Tz8djVvnmjiax9YNuEsljVzsmjo6ONc28DYQSCUs2+EyqQvH2lABK6uGPlBHM6j7/P6OHa+Y9omSgVQoVcmxCXzc/jUleVcviBn7M7KhAgI/U9fP0lvvz8Y/pgIS4qsAdbQvHxjDF95bC+NnX38xZXzB/VfVJA+6MFwqqmbNJdzQvMKAty4soikBAd3P7qb2tYePr9hIfkZSVy+IJen9lrhm51nWllre+xzc1LJTXPxzukWnj94Ho/PzyXFw8Mw6+Zl4zfWAh//tuUQ71qYy6ZL5kzYztV2fn5o+CYg9KebuofNaAZr4HdlqXtYzD+UwowknA4Z5NEfrevE6zfTVvoggAq9MiFSXE7uu3lZ2HQ+ZWoosz3AFw7VU+xOZn3IRK7xclGx5TE+sLWKHo81GPjzN07x9N5zfPmGJVy2YPBAbkVBOm09/TR2emjp8vDcgTqWFmdOKryQkZzIDcuLqG7uYUlhBu+2s4U+sKqEk41dvHConoaOvuDsXBHh4nnZ7DrTwtN7z1KalcJC93DJWjM3CxH42pP78fkN3/yTVZOy86LiDFxOR7DwnN9v2FPdGsw4GurVt3X3s/NMy6hhG7AmrxVlJg/y6PfbM2KnM7USVOgVJWpJdSUE01hvXl0y6qpdY7F2ThafurLcKrT1n6/y/7ad5htbDnHdRQV8LkxGUEWBlZJ59HwHdz+6m6ZOD/9yc+RZSyPxkfVWCujnNywMivFGe8bw/33mkG3rwAPt4rnZnGjs4tVjjXxgVXFYAc9MTmRxQQYer5+/v2EJc3MnHl4CK/V1WUlmcLzgeEMnHX1ePn75PAD21LQO6v/68Ub8xipoNxalQ1Is99a0kZGcEJwgN12o0CtKFFNml0IYbTGWSBAR7rt5GQ//5eX0+/z805P7KXIn852PrAn7AKkotDJv/vWpg2w90sDXPnARK8sm73VeXZHPH//2Gm4NyR7KSXPxroW5nGjoIinBwdKQZSQDcXqv3wyaKDaUP7m4lBuWF3KHvZzkZFkzJ4t9NW14ff5g2ObaJfnMy01l35DaOi8faSAzOYHVZVljXrcsZNJUv8/P8wfruGJB7rQOxEKEQi8iG0XkiIhUicg9YY5fIyI7RcQrIrcNOXaHiByzP3dMleGKciGw0p58NFWDdVcszOXZL13D39+whJ9+8pJgHf6hFGQkkZGcwOG6Dt6/sjjozU4FiwszhgnbB2wRX1nqHjTgvKrMTYJDmJuTyspR4tifvXYhP/r4+kGT0ibDmjlZ9PT7OFbfya7qFtwpiZTnprGy1D0odGOM4eWjDVxdkT9iemsoJVkp1LX34vVZVUcbOz18ZP3ExxMiZczEYhFxAg8A7wNqgO0istkYczCk2xngk8CXh5ybA/wzsB4wwDv2uaNXIVIUBYB/uWU5/T7/lHp86UkJfOHd4WdvBhARVpa6Odvawzc/vHLaPc4blhfxT78/wCXlgwf3kxOdfObqBVQUpE+7DaGsCRmQDc7UdQirytw8tfccjZ195KUnsf1UC3XtvVwbQdgGrNCNz2+oa+/ld+/UkJvmiijkM1kimUFyKVBljDkBICKPALcCQaE3xpyyjw2dNnYD8Lwxptk+/jywEXh40pYrygWA0yE4HZOfuDUR/vvPL0ZEZmTAPSvVxTNfvJqizOF57/fcuHTa7z+UebmpZKUm8lpVI0fOd7DRLha3yg7P7Ktt491LCnjwFWth+cCyimMRKKC2v7aNFw+f5xNXzJ+RMt+RCH0pEFpztAa4LMLrhzt3WI6YiNwJ3AlQWFhIZWVlhJcfTmdn56TOnwnUxqkjFuyMBRshOuwcq7jxTNo4J9XPs/vOYQw4W85QWXmWHq9BgM2v7ubs0QReONTDBxcl8tYbr0Zk57lOyxf+1ubd9PsM800dlZXhF5OfSiKfEz6NGGMeBB4EWL9+vdmwYcOEr1VZWclkzp8J1MapIxbsjAUbITbsnEkbd/UfZZ+9SMkn3n9tcDxjwZ5KOhLT2NXrIjnxLPfdvmFYzaeR7Ozt93Hva89yst0qSf3xm6+e9p8DIhuMrQVCRwvK7LZImMy5iqIos0YgTr8gP23QoPWqsizePtnM73fXsumSueMq7Jec6AymzN42QrXR6SASod8OVIhIuYi4gE3A5giv/xxwvYhki0g2cL3dpiiKEtUEZsiG5vWDlQnU3uvFbxi0sHyklGalkOgUbpnETOfxMmboxhjjFZG7sATaCTxkjDkgIvcDO4wxm0XkEuAJIBu4WUS+boxZboxpFpF/xXpYANwfGJhVFEWJZnLSXPzLzcuGzRoOVBn9wKriEauSjsZHL5lDS5dnRkt8RxSjN8ZsAbYMabsvZHs7Vlgm3LkPAQ9NwkZFUZRZ4ZNXDvfYV5Vlcec1CyY8t+DPL5u6OQmREhWDsYqiKLFCotPBV2+6aLbNGBdaAkFRFCXOUaFXFEWJc1ToFUVR4hwVekVRlDhHhV5RFCXOUaFXFEWJc1ToFUVR4hwVekVRlDhHAqu8Rwsi0gCcnsQl8oDGKTJnulAbp45YsDMWbITYsDMWbITZsXOeMSbsKiZRJ/STRUR2GGPWz7Ydo6E2Th2xYGcs2AixYWcs2AjRZ6eGbhRFUeIcFXpFUZQ4Jx6F/sHZNiAC1MapIxbsjAUbITbsjAUbIcrsjLsYvaIoijKYePToFUVRlBBU6BVFUeKcuBF6EdkoIkdEpEpE7pllWx4SkXoR2R/SliMiz4vIMfvfbLtdROQ/bbv3isjFM2TjHBHZKiIHReSAiHwx2uwUkWQReVtE9tg2ft1uLxeRt2xbfmOvZYyIJNn7Vfbx+dNtY4itThHZJSJPRbGNp0Rkn4jsFpEddlvUfN8hdmaJyO9E5LCIHBKRK6LJThFZYv8OA592EflSNNk4DGNMzH+w1rI9DiwAXMAeYNks2nMNcDGwP6Tt34F77O17gG/Z2zcBzwACXA68NUM2FgMX29sZwFFgWTTZad8r3d5OBN6y7/0osMlu/yHweXv7r4Af2tubgN/M4Hd+N/Br4Cl7PxptPAXkDWmLmu87xKafA5+xt11AVjTaad/fCdQB86LVRmNM3Aj9FcBzIfv3AvfOsk3zhwj9EaDY3i4GjtjbPwJuD9dvhu39PfC+aLUTSAV2ApdhzThMGPrdYy1gf4W9nWD3kxmwrQx4EXgP8JT9HzqqbLTvF07oo+r7BtzAyaG/k2izM+R+1wOvR7ONxpi4Cd2UAtUh+zV2WzRRaIw5Z2/XAYX29qzbbocP1mJ5zFFlpx0S2Q3UA89jvbm1GmO8YewI2mgfbwNyp9tG4HvAPwB+ez83Cm0EMMAfReQdEbnTbouq7xsoBxqAn9qhsB+LSFoU2hlgE/CwvR2tNsaN0McUxnqsR0Veq4ikA48BXzLGtIceiwY7jTE+Y8waLK/5UmDpbNozFBH5AFBvjHlntm2JgKuMMRcDNwJfEJFrQg9Gw/eN9ZZzMfA/xpi1QBdWGCRIlNiJPe5yC/DboceixcYA8SL0tcCckP0yuy2aOC8ixQD2v/V2+6zZLiKJWCL/K2PM49FqJ4AxphXYihUGyRKRhDB2BG20j7uBpmk27UrgFhE5BTyCFb75fpTZCIAxptb+tx54AuvBGW3fdw1QY4x5y97/HZbwR5udYD0wdxpjztv70WgjED9Cvx2osDMdXFivU5tn2aahbAbusLfvwIqJB9o/YY/MXw60hbz+TRsiIsBPgEPGmO9Go50iki8iWfZ2CtYYwiEswb9tBBsDtt8GvGR7VtOGMeZeY0yZMWY+1t/dS8aYP48mGwFEJE1EMgLbWLHl/UTR9w1gjKkDqkVkid30XuBgtNlpczsDYZuALdFmo8VMDghM86DITViZI8eBf5xlWx4GzgH9WB7Kp7HisC8Cx4AXgBy7rwAP2HbvA9bPkI1XYb1a7gV225+boslOYBWwy7ZxP3Cf3b4AeBuownptTrLbk+39Kvv4ghn+3jcwkHUTVTba9uyxPwcC/0ei6fsOsXUNsMP+3p8EsqPNTiAN603MHdIWVTaGfrQEgqIoSpwTL6EbRVEUZQRU6BVFUeIcFXpFUZQ4R4VeURQlzlGhVxRFiXNU6JUpw646+FcTPHdLIGd+lD73i8h1EzJu+LW+OmT/jam4rn2tD4rIfVN1vckiIpUiMuJC1SLy/4nIe2bSJmVm0fRKZcqwa+Y8ZYxZEeZYghmo/TLriEinMSZ9mq79BnCLMaZxOq4/XkSkEviyMWbHCMfnAf9rjLl+Rg1TZgz16JWp5JvAQrtG97dFZIOIvCoim7FmNyIiT9pFtQ6EFNYK1ErPE5H5YtUg/1+7zx/tWbGIyM9E5LaQ/l8XkZ1i1Vhfarfn27XAD9gFsU6LSF6okSLyTSDFtvNXdlun/e8GEXlZRH4vIidE5Jsi8udi1cXfJyILQ+7zmIhstz9X2u2Lgb6AyIvIR0Rkv1g19V+x25z272e7WPXJPxti21fs++yx7URE1ojINrvvEzJQ57xSRL5l23ZURK6221NE5BH79/gEkBJy35/Z9uwTkb8FMMacBnJFpGiq/hCUKGOmZ2jpJ34/DC/NvAGrKFV5SFtgtmAK1mzXXHv/FJBnX8MLrLHbHwU+Zm//DLgtpP9f29t/BfzY3v4v7BLVwEas2b95YWztDLdv29yKVWY2CasmydftY18Evmdv/xqrSBjAXKxSEgB/AXwn5Lr7gFJ7O8v+907ga/Z2EtYs0HKs2ilvAKlDfld7gWvt7ftDbKgM3AtrVvML9vbdwEP29ir797keWAc8H2JbVsj2/wIfnu2/If1MzydQdElRpou3jTEnQ/b/RkQ+ZG/PASoYXtTrpDFmt739Dpb4h+PxkD5/Ym9fBXwIwBjzrIi0TMDm7cauRSIix4E/2u37gHfb29cBy0QkcE6mWJVAi7HK7AZ4HfiZiDwaYu/1wKrA2wlWYbMK+5o/NcZ02/Y3i4gbS5Bftvv+nMHVEkN/B/Pt7WuA/7SvsVdE9trtJ4AFIvID4OmQnwusAlwlY/xelBhFhV6ZbroCGyKyAUvMrjDGdNux4+Qw5/SFbPuwQw+j9PMxtX/Loff3h+z7Q+7jAC43xvSGnigiPVjCDYAx5nMichnwfuAdEVmHVfvkr40xzw0594ZJ2Drm78AY0yIiq4EbgM8BHwU+ZR9OBnomcH8lBtAYvTKVdGAtSzgSbqDFFvmlWMuqTTWvYwkYInI9VkGscPSLVaZ5ovwR+OvAjoissTcPAYtC2hcaY94yxtyH5enPwVpl6vOB+4vIYrEqSj4P/IWIpNrtOcaYNqAlEH8HPg4EvPuReAX4M/saK7DCN9hjFQ5jzGPA17DK/wZYjBVKU+IQ9eiVKcMY0yQir4u1KPozWOGBUJ4FPicih7CWU9s2DWZ8HXhYRD4OvIm10k9HmH4PAntFZKexygqPl78BHrDDIglY4vo5+9/viIgYYwzwbRGpwPLiX8SqHrkXK8yyU6zYTwPwQTvUtAbYISIeYAvwVayStz+0HwAnsMYBRuN/sFZoOoT14AksilJqtwccvHshuC7BIqyxAiUO0fRKJa4QkSTAZ4zxisgVWCsVrZlhG74P/MEY88JM3nei2GMmFxtj/mm2bVGmB/XolXhjLvCo7bV6gL+cBRv+DWsR81ghAfjObBuhTB/q0SuKosQ5OhirKIoS56jQK4qixDkq9IqiKHGOCr2iKEqco0KvKIoS5/z/QU816DoQRcoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [12:21<00:00, 1348.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-core. Took 741.5599095869984 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = default_timer()\n",
    "for t in trange(max_steps):\n",
    "    batch_ix = np.random.randint(0, len(cooc.row), size=batch_size)\n",
    "    ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
    "    \n",
    "    # Compute gradients\n",
    "    emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
    "      model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
    "    \n",
    "    grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
    "        emb_ii, emb_jj, bias_ii, bias_jj, counts_ij)\n",
    "    \n",
    "    # SGD step\n",
    "    model.embeddings[ii] -= learning_rate * grad_emb_ii\n",
    "    model.embeddings[jj] -= learning_rate * grad_emb_jj\n",
    "    model.biases[ii] -= learning_rate * grad_bias_ii\n",
    "    model.biases[jj] -= learning_rate * grad_bias_jj\n",
    "    \n",
    "    if t % 10_000 == 0:\n",
    "        batch_ix = np.random.randint(0, len(cooc.row), size=4096)\n",
    "        ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
    "        emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
    "            model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
    "        \n",
    "        loss = compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij)\n",
    "        print(f'Iter :{t},\\t, process: {os.getpid()} Loss: {loss}')\n",
    "\n",
    "\n",
    "        timestep_history.append(time.perf_counter() - start_time)\n",
    "        loss_history.append(compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij))\n",
    "        clear_output(True)\n",
    "        plt.plot(timestep_history, loss_history)\n",
    "        plt.xlabel('training time(seconds)')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "end = default_timer()\n",
    "print(f'Single-core. Took {end - start} seconds...')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noZiGvffAFE8"
   },
   "source": [
    "__Now let's parallelize it!__\n",
    "\n",
    "The code above is cute, but it only uses one CPU core. Surely we can go faster!\n",
    "\n",
    "The main challenge in this week's seminar is to speed up GloVe training by all means necessary.\n",
    "\n",
    "Here's what you should do:\n",
    "* make multiple parallel workers, each training your model on different random data,\n",
    "* build some centralized means of progress tracking: track the average loss and the number of training steps,\n",
    "* implement workers in such a way that no process is left hanging after the training is over.\n",
    "\n",
    "\n",
    "Finally, please compare the loss / training time plot of your algorithm against the baseline.\n",
    "\n",
    "_Notes:_\n",
    "* Remember to set a different np.random.seed in each worker!\n",
    "* You can track the training progress either via mp.Pipe or via shared variables\n",
    "* It is better to separate training and plotting into different processes\n",
    "* If you want to prevent concurrent updates to shared memory, you can use [mp.Lock](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Lock) or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uTD4WQtjAFE9"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "max_steps = 10 ** 6\n",
    "start_time = time.perf_counter()\n",
    "timestep_history = []\n",
    "loss_history = []\n",
    "\n",
    "model = SharedEmbeddings(vocabulary_size, embedding_dimension=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(batch_ix, t, output_queue):\n",
    "    ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
    "    \n",
    "    # Compute gradients\n",
    "    emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
    "      model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
    "    \n",
    "    grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
    "        emb_ii, emb_jj, bias_ii, bias_jj, counts_ij)\n",
    "    \n",
    "    # SGD step\n",
    "    model.embeddings[ii] -= learning_rate * grad_emb_ii\n",
    "    model.embeddings[jj] -= learning_rate * grad_emb_jj\n",
    "    model.biases[ii] -= learning_rate * grad_bias_ii\n",
    "    model.biases[jj] -= learning_rate * grad_bias_jj\n",
    "    if t % 10_000 == 0:\n",
    "        batch_ix = np.random.randint(0, len(cooc.row), size=4096)\n",
    "        ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
    "        emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
    "            model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
    "        loss = compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij)\n",
    "        print(f'Iter :{t},\\t, process: {os.getpid()} Loss: {loss}')\n",
    "\n",
    "        # lock.acquire()\n",
    "        # timestep_history.append()\n",
    "        time_stamp = time.perf_counter() - start_time\n",
    "        output_queue.put((time_stamp, loss))\n",
    "        # lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(stack, output_queue):\n",
    "    while stack:\n",
    "        t, batch_ix = stack.get()\n",
    "        if t=='kill':\n",
    "            print(f'Process {os.getpid()} is killed\\n')\n",
    "            return\n",
    "        update_step(batch_ix, t, output_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ix = zip(list(range(max_steps)), np.random.randint(0, len(cooc.row), size=(max_steps, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "n_cpu = mp.cpu_count()\n",
    "stack = deque(batch_ix)\n",
    "stack.extend([('kill','kill')]*n_cpu)\n",
    "lock = mp.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue.close()\n",
    "# queue.join_thread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = mp.Queue()\n",
    "for el in stack:\n",
    "    queue.put(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_queue = mp.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter :0,\t, process: 3670 Loss: 0.5339243574731565\n",
      "Iter :10000,\t, process: 3673 Loss: 0.4777524275459558\n",
      "Iter :20000,\t, process: 3673 Loss: 0.41342607783450314\n",
      "Iter :30000,\t, process: 3678 Loss: 0.39698008237535076\n",
      "Iter :40000,\t, process: 3670 Loss: 0.3509582747338436\n",
      "Iter :50000,\t, process: 3677 Loss: 0.3404389854344658\n",
      "Iter :60000,\t, process: 3677 Loss: 0.3068867965646699\n",
      "Iter :70000,\t, process: 3673 Loss: 0.2647449147981713\n",
      "Iter :80000,\t, process: 3673 Loss: 0.2646971286601304\n",
      "Iter :90000,\t, process: 3677 Loss: 0.24129299906811744\n",
      "Iter :100000,\t, process: 3677 Loss: 0.24302798500508682\n",
      "Iter :110000,\t, process: 3677 Loss: 0.23585099140807023\n",
      "Iter :120000,\t, process: 3670 Loss: 0.21615565319396585\n",
      "Iter :130000,\t, process: 3670 Loss: 0.2198116959683177\n",
      "Iter :140000,\t, process: 3678 Loss: 0.21825839343749348\n",
      "Iter :150000,\t, process: 3673 Loss: 0.209469379701872\n",
      "Iter :160000,\t, process: 3673 Loss: 0.20409819866892895\n",
      "Iter :170000,\t, process: 3673 Loss: 0.20353471481968147\n",
      "Iter :180000,\t, process: 3678 Loss: 0.1851578593445229\n",
      "Iter :190000,\t, process: 3677 Loss: 0.18891992932369564\n",
      "Iter :200000,\t, process: 3677 Loss: 0.1916900203124279\n",
      "Iter :210000,\t, process: 3670 Loss: 0.18393077852473033\n",
      "Iter :220000,\t, process: 3677 Loss: 0.1585715736338235\n",
      "Iter :230000,\t, process: 3670 Loss: 0.17297913069226972\n",
      "Iter :240000,\t, process: 3678 Loss: 0.1766977346399134\n",
      "Iter :250000,\t, process: 3677 Loss: 0.15798376680807483\n",
      "Iter :260000,\t, process: 3670 Loss: 0.17474395607118415\n",
      "Iter :270000,\t, process: 3673 Loss: 0.1479003296584046\n",
      "Iter :280000,\t, process: 3678 Loss: 0.16521029161752593\n",
      "Iter :290000,\t, process: 3677 Loss: 0.1528552658066468\n",
      "Iter :300000,\t, process: 3673 Loss: 0.14840322453539356\n",
      "Iter :310000,\t, process: 3677 Loss: 0.1530483006052685\n",
      "Iter :320000,\t, process: 3677 Loss: 0.1824474477250797\n",
      "Iter :330000,\t, process: 3670 Loss: 0.1383042949263851\n",
      "Iter :340000,\t, process: 3678 Loss: 0.14546375112972476\n",
      "Iter :350000,\t, process: 3673 Loss: 0.14321348056811167\n",
      "Iter :360000,\t, process: 3677 Loss: 0.16723146809047884\n",
      "Iter :370000,\t, process: 3673 Loss: 0.1440832447941042\n",
      "Iter :380000,\t, process: 3678 Loss: 0.1538551675283778\n",
      "Iter :390000,\t, process: 3677 Loss: 0.1264505261470784\n",
      "Iter :400000,\t, process: 3678 Loss: 0.1299120318765114\n",
      "Iter :410000,\t, process: 3678 Loss: 0.1348232734022731\n",
      "Iter :420000,\t, process: 3677 Loss: 0.17474512327321715\n",
      "Iter :430000,\t, process: 3670 Loss: 0.13293543219361326\n",
      "Iter :440000,\t, process: 3677 Loss: 0.17092810312611206\n",
      "Iter :450000,\t, process: 3678 Loss: 0.13166219906877163\n",
      "Iter :460000,\t, process: 3678 Loss: 0.13395707146639968\n",
      "Iter :470000,\t, process: 3670 Loss: 0.12984111348893418\n",
      "Iter :480000,\t, process: 3670 Loss: 0.13205626358949818\n",
      "Iter :490000,\t, process: 3670 Loss: 0.16024169645412129\n",
      "Iter :500000,\t, process: 3670 Loss: 0.15149640052374022\n",
      "Iter :510000,\t, process: 3677 Loss: 0.14506497745723476\n",
      "Iter :520000,\t, process: 3670 Loss: 0.11735504534848477\n",
      "Iter :530000,\t, process: 3670 Loss: 0.16350803328369726\n",
      "Iter :540000,\t, process: 3673 Loss: 0.15568411778133862\n",
      "Iter :550000,\t, process: 3678 Loss: 0.15486703450805234\n",
      "Iter :560000,\t, process: 3670 Loss: 0.15835500965463417\n",
      "Iter :570000,\t, process: 3677 Loss: 0.13627617392496205\n",
      "Iter :580000,\t, process: 3673 Loss: 0.14541272796998073\n",
      "Iter :590000,\t, process: 3678 Loss: 0.14474948190145231\n",
      "Iter :600000,\t, process: 3670 Loss: 0.13882938840313902\n",
      "Iter :610000,\t, process: 3670 Loss: 0.1337463908044438\n",
      "Iter :620000,\t, process: 3670 Loss: 0.12991749620040682\n",
      "Iter :630000,\t, process: 3670 Loss: 0.10112680944253348\n",
      "Iter :640000,\t, process: 3673 Loss: 0.11190874227346331\n",
      "Iter :650000,\t, process: 3673 Loss: 0.15468219323408644\n",
      "Iter :660000,\t, process: 3677 Loss: 0.128197029296452\n",
      "Iter :670000,\t, process: 3673 Loss: 0.15025451466179496\n",
      "Iter :680000,\t, process: 3670 Loss: 0.11522117598410976\n",
      "Iter :690000,\t, process: 3673 Loss: 0.13407247040339065\n",
      "Iter :700000,\t, process: 3673 Loss: 0.1290004011819459\n",
      "Iter :710000,\t, process: 3677 Loss: 0.09792449674894374\n",
      "Iter :720000,\t, process: 3677 Loss: 0.11346445057225574\n",
      "Iter :730000,\t, process: 3673 Loss: 0.12540072791961446\n",
      "Iter :740000,\t, process: 3677 Loss: 0.13201525269746664\n",
      "Iter :750000,\t, process: 3678 Loss: 0.10815432794248928\n",
      "Iter :760000,\t, process: 3670 Loss: 0.1309146617483601\n",
      "Iter :770000,\t, process: 3670 Loss: 0.14043791388093357\n",
      "Iter :780000,\t, process: 3670 Loss: 0.11027912251377242\n",
      "Iter :790000,\t, process: 3677 Loss: 0.13944277243856135\n",
      "Iter :800000,\t, process: 3673 Loss: 0.09500839163584579\n",
      "Iter :810000,\t, process: 3670 Loss: 0.12357210316298958\n",
      "Iter :820000,\t, process: 3670 Loss: 0.10739840219434638\n",
      "Iter :830000,\t, process: 3677 Loss: 0.10901442430467437\n",
      "Iter :840000,\t, process: 3670 Loss: 0.14239800088154303\n",
      "Iter :850000,\t, process: 3670 Loss: 0.11281907066669923\n",
      "Iter :860000,\t, process: 3673 Loss: 0.1085133634632513\n",
      "Iter :870000,\t, process: 3673 Loss: 0.12598807565404135\n",
      "Iter :880000,\t, process: 3677 Loss: 0.12131224594792155\n",
      "Iter :890000,\t, process: 3670 Loss: 0.11072645451392425\n",
      "Iter :900000,\t, process: 3678 Loss: 0.1425121441556278\n",
      "Iter :910000,\t, process: 3678 Loss: 0.13902813465611547\n",
      "Iter :920000,\t, process: 3673 Loss: 0.1337833347033342\n",
      "Iter :930000,\t, process: 3677 Loss: 0.10447304645611802\n",
      "Iter :940000,\t, process: 3673 Loss: 0.10681955165773285\n",
      "Iter :950000,\t, process: 3678 Loss: 0.12530172930484876\n",
      "Iter :960000,\t, process: 3670 Loss: 0.11485905718262479\n",
      "Iter :970000,\t, process: 3677 Loss: 0.13717504601454514\n",
      "Iter :980000,\t, process: 3670 Loss: 0.1053055698422978\n",
      "Iter :990000,\t, process: 3670 Loss: 0.1316121322493713\n",
      "Process 3678 is killed\n",
      "Process 3677 is killed\n",
      "Process 3670 is killed\n",
      "Process 3673 is killed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Multi-core. Took 325.0457204740087 seconds...\n"
     ]
    }
   ],
   "source": [
    "start = default_timer()\n",
    "ls = []\n",
    "for n in range(n_cpu):\n",
    "    proc = mp.Process(target=worker, args=[queue, output_queue])\n",
    "    ls.append(proc)\n",
    "    proc.start()\n",
    "\n",
    "for proc in ls:\n",
    "    proc.join()\n",
    "end = default_timer()\n",
    "print(f'Multi-core. Took {end - start} seconds...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_history = []\n",
    "loss_history = []\n",
    "\n",
    "while not output_queue.empty():\n",
    "    stamp, loss = output_queue.get()\n",
    "    timestep_history.append(stamp)\n",
    "    loss_history.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(timestep_history, loss_history)\n",
    "plt.xlabel('training time(seconds)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME0lZHuKAFE-"
   },
   "source": [
    "Hello, I'm `Misha` and here's what i've done:\n",
    "\n",
    "* I created the list of indexes and then put into mp.Queue\n",
    "* After that I just used mp.Pool and mp.Queue, pretty easy task to be honest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "practice.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
